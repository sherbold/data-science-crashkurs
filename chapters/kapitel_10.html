
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>10. Text Mining &#8212; Data Science Crashkurs - Eine interaktive und praktische Einführung</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="11. Statistik" href="kapitel_11.html" />
    <link rel="prev" title="9. Zeitreihenanalyse" href="kapitel_09.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/bookcover.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Data Science Crashkurs - Eine interaktive und praktische Einführung</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="vorwort.html">
                    Vorwort
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Kapitel
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="kapitel_01.html">
   1. Big Data und Data Science
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kapitel_02.html">
   2. Der Prozess von Data-Science-Projekten
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kapitel_03.html">
   3. Allgemeines zur Datenanalyse
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kapitel_04.html">
   4. Erkunden der Daten
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kapitel_05.html">
   5. Assoziationsregeln
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kapitel_06.html">
   6. Clusteranalyse
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kapitel_07.html">
   7. Klassifikation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kapitel_08.html">
   8. Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kapitel_09.html">
   9. Zeitreihenanalyse
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   10. Text Mining
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kapitel_11.html">
   11. Statistik
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kapitel_12.html">
   12. Big Data Processing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kapitel_13.html">
   13. Weiterführende Konzepte
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Appendix
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="howto.html">
   Selbst ausführen
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notations.html">
   Notationen
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="acronyms.html">
   Abkürzungen
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Gefällt Ihnen das Buch? Möchten Sie es in den Händen halten und weitere Open Access Bücher unterstützen? <a href="https://dpunkt.de/produkt/data-science-crashkurs/">Dann kaufen Sie die Print Edition.</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/sherbold/einfuehrung-in-data-science/main?urlpath=tree/content/chapters/kapitel_10.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>
<a href="https://github.com/sherbold/einfuehrung-in-data-science"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="bottom"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/chapters/kapitel_10.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#preprocessing">
   10.1. Preprocessing
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#erstellung-eines-korpus">
     10.1.1. Erstellung eines Korpus
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#relevanter-inhalt">
     10.1.2. Relevanter Inhalt
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#zeichensetzung-und-groszschreibung">
     10.1.3. Zeichensetzung und Großschreibung
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#stoppworter">
     10.1.4. Stoppwörter
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#stemming-und-lemmatisierung">
     10.1.5. Stemming und Lemmatisierung
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#visualisierung-des-preprocessings">
     10.1.6. Visualisierung des Preprocessings
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bag-of-words">
     10.1.7. Bag-of-Words
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#inverse-document-frequency">
     10.1.8. Inverse Document Frequency
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#jenseits-des-bag-of-words">
     10.1.9. Jenseits des Bag-of-Words
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#herausforderungen-des-text-mining">
   10.2. Herausforderungen des Text Mining
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dimensionalitat">
     10.2.1. Dimensionalität
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mehrdeutigkeiten">
     10.2.2. Mehrdeutigkeiten
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#weitere-probleme">
     10.2.3. Weitere Probleme
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ubung">
   10.3. Übung
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#wortwolke-ohne-preprocessing">
     10.3.1. Wortwolke ohne Preprocessing
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#wortwolke-mit-preprocessing">
     10.3.2. Wortwolke mit Preprocessing
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tf-idf">
     10.3.3. TF-IDF
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Text Mining</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#preprocessing">
   10.1. Preprocessing
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#erstellung-eines-korpus">
     10.1.1. Erstellung eines Korpus
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#relevanter-inhalt">
     10.1.2. Relevanter Inhalt
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#zeichensetzung-und-groszschreibung">
     10.1.3. Zeichensetzung und Großschreibung
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#stoppworter">
     10.1.4. Stoppwörter
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#stemming-und-lemmatisierung">
     10.1.5. Stemming und Lemmatisierung
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#visualisierung-des-preprocessings">
     10.1.6. Visualisierung des Preprocessings
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bag-of-words">
     10.1.7. Bag-of-Words
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#inverse-document-frequency">
     10.1.8. Inverse Document Frequency
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#jenseits-des-bag-of-words">
     10.1.9. Jenseits des Bag-of-Words
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#herausforderungen-des-text-mining">
   10.2. Herausforderungen des Text Mining
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dimensionalitat">
     10.2.1. Dimensionalität
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mehrdeutigkeiten">
     10.2.2. Mehrdeutigkeiten
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#weitere-probleme">
     10.2.3. Weitere Probleme
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ubung">
   10.3. Übung
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#wortwolke-ohne-preprocessing">
     10.3.1. Wortwolke ohne Preprocessing
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#wortwolke-mit-preprocessing">
     10.3.2. Wortwolke mit Preprocessing
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tf-idf">
     10.3.3. TF-IDF
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="text-mining">
<h1><span class="section-number">10. </span>Text Mining<a class="headerlink" href="#text-mining" title="Permalink to this headline">#</a></h1>
<p>Beim <em>Text Mining</em> geht es um die Anwendung der in den vergangenen Kapiteln besprochenen Methoden auf textuelle Daten mit dem Ziel, Wissen aus den Daten zu gewinnen. Beispiele für Text Mining sind die Analyse von Kundenbewertungen bezüglich der Emotion und Meinung (engl. <em>sentiment</em>) oder die automatische Gruppierung ähnlicher Dokumente. Das Problem bei der Analyse von natürlicher Sprache ist, dass Sätze und längere Texte weder numerisch noch kategorisch sind. Es gibt also keine offensichtliche Darstellung durch Merkmale. Hinzu kommt, dass Text oft eine innere Struktur hat, zum Beispiel durch Überschriften, Einleitungen oder Referenzen zu verwandten Inhalten. Wenn wir Text lesen, erkennen wir diese semantische Struktur automatisch und ordnen diese ein. Daher ist es die große Herausforderung des Text Mining, eine geeignete Struktur der textuellen Daten für das maschinelle Lernen zu finden.</p>
<p>Hierfür müssen wir den Text <em>codieren</em>, um eine numerische oder kategorische Repräsentation zu erhalten. Ziel der Repräsentation sollte sein, möglichst wenig relevante Information aus dem Text zu verlieren. Eine ideale Codierung beinhaltet daher nicht nur die Worte, sondern auch die <em>Bedeutung</em> der Worte im jeweiligen <em>Kontext</em>, die grammatikalische Struktur und eventuell sogar den Gesamtkontext eines Satzes innerhalb eines Dokuments. Aufgrund dieser Komplexität ist die Repräsentation von Texten für das maschinelle Lernen immer noch der Gegenstand der aktuellen Forschung. Durch Fortschritte der letzten Jahre wird das Text Mining jedoch zu einem immer mächtigeren und zuverlässigeren Werkzeug. Das Text Mining selbst ist ein großes Gebiet, bei dem wir hier nur an der Oberfläche kratzen können. Das Ziel dieses Kapitels ist es, dass wir ein gutes Verständnis der Herausforderungen vom Text Mining bekommen, grundlegende Verfahren kennenlernen und außerdem Wissen wie fortgeschrittene Verfahren erarbeiten.</p>
<p>Als Beispiel nutzen wir in diesem Kapitel acht Tweets von Donald Trump. Die Verarbeitungsschritte, die wir zeigen, haben alle das Ziel, die Analyse des Themas der Tweets zu erlauben. Ähnlich wie bei den Bostondaten ist es auch bei Donald Trumps Tweets: Diese sind inhaltlich, vor allem was das Thema Wahlen angeht, nicht unproblematisch. Problematische Daten existieren aber und es ist wichtig, sich auch damit auseinanderzusetzen. Hinzu kommt, dass sie ein gutes Beispiel sind, um zu zeigen, wie Text Mining funktioniert: Sie sind kurz, jeder kennt das Medium, die Auswahl an Tweets ist inhaltlich unkritisch und die Daten beinhalten die üblichen Probleme, auf die man beim Text Mining trifft.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">textwrap</span> <span class="kn">import</span> <span class="n">TextWrapper</span>

<span class="n">tweets_list</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Oct 4, 2018 08:03:25 PM Beautiful evening in Rochester, Minnesota. VOTE, VOTE, VOTE! https://t.co/SyxrxvTpZE [Twitter for iPhone]&#39;</span><span class="p">,</span>
               <span class="s1">&#39;Oct 4, 2018 07:52:20 PM Thank you Minnesota - I love you! https://t.co/eQC2NqdIil [Twitter for iPhone]&#39;</span><span class="p">,</span>
               <span class="s1">&#39;Oct 4, 2018 05:58:21 PM Just made my second stop in Minnesota for a MAKE AMERICA GREAT AGAIN rally. We need to elect @KarinHousley to the U.S. Senate, and we need the strong leadership of @TomEmmer, @Jason2CD, @JimHagedornMN and @PeteStauber in the U.S. House! [Twitter for iPhone]&#39;</span><span class="p">,</span>
               <span class="s1">&#39;Oct 4, 2018 05:17:48 PM Congressman Bishop is doing a GREAT job! He helped pass tax reform which lowered taxes for EVERYONE! Nancy Pelosi is spending hundreds of thousands of dollars on his opponent because they both support a liberal agenda of higher taxes and wasteful spending! [Twitter for iPhone]&#39;</span><span class="p">,</span>
               <span class="s1">&#39;Oct 4, 2018 02:29:27 PM &quot;U.S. Stocks Widen Global Lead&quot; https://t.co/Snhv08ulcO [Twitter for iPhone]&#39;</span><span class="p">,</span>
               <span class="s1">&#39;Oct 4, 2018 02:17:28 PM Statement on National Strategy for Counterterrorism: https://t.co/ajFBg9Elsj https://t.co/Qr56ycjMAV [Twitter for iPhone]&#39;</span><span class="p">,</span>
               <span class="s1">&#39;Oct 4, 2018 12:38:08 PM Working hard, thank you! https://t.co/6HQVaEXH0I [Twitter for iPhone]&#39;</span><span class="p">,</span>
               <span class="s1">&#39;Oct 4, 2018 09:17:01 AM This is now the 7th. time the FBI has investigated Judge Kavanaugh. If we made it 100, it would still not be good enough for the Obstructionist Democrats. [Twitter for iPhone]&#39;</span><span class="p">]</span>

<span class="n">wrapper</span> <span class="o">=</span> <span class="n">TextWrapper</span><span class="p">(</span><span class="n">width</span><span class="o">=</span><span class="mi">65</span><span class="p">)</span>
<span class="k">for</span> <span class="n">tweet</span> <span class="ow">in</span> <span class="n">tweets_list</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">wrapper</span><span class="o">.</span><span class="n">wrap</span><span class="p">(</span><span class="n">tweet</span><span class="p">)))</span>
    <span class="nb">print</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Oct 4, 2018 08:03:25 PM Beautiful evening in Rochester,
Minnesota. VOTE, VOTE, VOTE! https://t.co/SyxrxvTpZE [Twitter for
iPhone]

Oct 4, 2018 07:52:20 PM Thank you Minnesota - I love you!
https://t.co/eQC2NqdIil [Twitter for iPhone]

Oct 4, 2018 05:58:21 PM Just made my second stop in Minnesota for
a MAKE AMERICA GREAT AGAIN rally. We need to elect @KarinHousley
to the U.S. Senate, and we need the strong leadership of
@TomEmmer, @Jason2CD, @JimHagedornMN and @PeteStauber in the U.S.
House! [Twitter for iPhone]

Oct 4, 2018 05:17:48 PM Congressman Bishop is doing a GREAT job!
He helped pass tax reform which lowered taxes for EVERYONE! Nancy
Pelosi is spending hundreds of thousands of dollars on his
opponent because they both support a liberal agenda of higher
taxes and wasteful spending! [Twitter for iPhone]

Oct 4, 2018 02:29:27 PM &quot;U.S. Stocks Widen Global Lead&quot;
https://t.co/Snhv08ulcO [Twitter for iPhone]

Oct 4, 2018 02:17:28 PM Statement on National Strategy for
Counterterrorism: https://t.co/ajFBg9Elsj https://t.co/Qr56ycjMAV
[Twitter for iPhone]

Oct 4, 2018 12:38:08 PM Working hard, thank you!
https://t.co/6HQVaEXH0I [Twitter for iPhone]

Oct 4, 2018 09:17:01 AM This is now the 7th. time the FBI has
investigated Judge Kavanaugh. If we made it 100, it would still
not be good enough for the Obstructionist Democrats. [Twitter for
iPhone]
</pre></div>
</div>
</div>
</div>
<section id="preprocessing">
<h2><span class="section-number">10.1. </span>Preprocessing<a class="headerlink" href="#preprocessing" title="Permalink to this headline">#</a></h2>
<p>Durch das Preprocessing wird der Text in eine Repräsentation überführt, die für maschinelles Lernen geeignet ist, also mit der man den Text klassifizieren oder clustern kann.</p>
<section id="erstellung-eines-korpus">
<h3><span class="section-number">10.1.1. </span>Erstellung eines Korpus<a class="headerlink" href="#erstellung-eines-korpus" title="Permalink to this headline">#</a></h3>
<p>Der erste Preprocessing-Schritt ist die Erstellung eines <em>Korpus</em> von <em>Dokumenten</em>. Im Sinne der in <a class="reference internal" href="kapitel_03.html"><span class="doc std std-doc">Kapitel 3</span></a> eingeführten Begriffe sind die Dokumente die Objekte und ein Korpus eine Menge von Objekten. In unserem Twitter-Beispiel ist der Korpus eine Menge von Tweets, jeder Tweet ist ein Dokument. Wir haben bereits eine Liste von Tweets, was einem Korpus entspricht. In anderen Anwendungsfäl-len kann die Erstellung des Korpus aufwendiger sein. Dies wäre zum Beispiel der Fall, wenn erst Bewertungen eines Produkts durch Crawling aus dem Internet gesammelt werden müssen. Hierbei ist es relativ wahrscheinlich, dass es auf der gleichen Webseite mehrere Bewertungen gibt. Diese müssen dann zum Beispiel innerhalb der Webseite identifiziert und anschließend in separate Dokumente aufgeteilt werden.</p>
</section>
<section id="relevanter-inhalt">
<h3><span class="section-number">10.1.2. </span>Relevanter Inhalt<a class="headerlink" href="#relevanter-inhalt" title="Permalink to this headline">#</a></h3>
<p>Die textuellen Daten beinhalten oft irrelevante Informationen für einen bestimmten Anwendungsfall, insbesondere wenn der Text automatisch aus dem Internet gesammelt wurde. Wenn wir am Thema von Tweets interessiert sind, ist der Zeitstempel irrelevant. Es ist ebenfalls nicht wichtig, ob ein Tweet mit einem iPhone oder einer anderen Anwendung verschickt wurde. Links zu Webseiten sind knifflig, da sie relevante Informationen enthalten könnten, jedoch auch irrelevant sein können. Wenn eine URL relevante Informationen, wie zum Beispiel den Autor oder das Thema, enthält, kann dies wertvoll für das Text Mining sein. Andere Aspekte, wie zum Beispiel das http(s):// zu Beginn eines Links, sind irrelevant. Es gibt auch Links, die keine relevanten Informationen enthalten, zum Beispiel wenn Link Shortener benutzt werden. Dann ist der Link nur eine zufällige Zeichenkette. Ob Links beibehalten werden sollen, muss man daher von Anwendungsfall zu Anwendungsfall entscheiden.</p>
<p>Wenn wir den irrelevanten Inhalt der Tweets (inkl. Links) entfernen, bekommen wir Folgendes:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">re</span>

<span class="n">tweets_relevant_content</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">tweet</span> <span class="ow">in</span> <span class="n">tweets_list</span><span class="p">:</span>
    <span class="c1"># remove the first 24 chars, because they are the time stamp</span>
    <span class="c1"># remove everything after last [ because this is the source of the tweet</span>
    <span class="n">modified_tweet</span> <span class="o">=</span> <span class="n">tweet</span><span class="p">[</span><span class="mi">24</span><span class="p">:</span><span class="n">tweet</span><span class="o">.</span><span class="n">rfind</span><span class="p">(</span><span class="s1">&#39;[&#39;</span><span class="p">)]</span>
    <span class="c1"># drop links</span>
    <span class="n">modified_tweet</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;http\S+&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">modified_tweet</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
    <span class="n">tweets_relevant_content</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">modified_tweet</span><span class="p">)</span>

<span class="k">for</span> <span class="n">tweet</span> <span class="ow">in</span> <span class="n">tweets_relevant_content</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">wrapper</span><span class="o">.</span><span class="n">wrap</span><span class="p">(</span><span class="n">tweet</span><span class="p">)))</span>
    <span class="nb">print</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Beautiful evening in Rochester, Minnesota. VOTE, VOTE, VOTE!

Thank you Minnesota - I love you!

Just made my second stop in Minnesota for a MAKE AMERICA GREAT
AGAIN rally. We need to elect @KarinHousley to the U.S. Senate,
and we need the strong leadership of @TomEmmer, @Jason2CD,
@JimHagedornMN and @PeteStauber in the U.S. House!

Congressman Bishop is doing a GREAT job! He helped pass tax
reform which lowered taxes for EVERYONE! Nancy Pelosi is spending
hundreds of thousands of dollars on his opponent because they
both support a liberal agenda of higher taxes and wasteful
spending!

&quot;U.S. Stocks Widen Global Lead&quot;

Statement on National Strategy for Counterterrorism:

Working hard, thank you!

This is now the 7th. time the FBI has investigated Judge
Kavanaugh. If we made it 100, it would still not be good enough
for the Obstructionist Democrats.
</pre></div>
</div>
</div>
</div>
<p>Was relevant und irrelevant ist, hängt auch vom Kontext ab. Ein anderer Anwendungsfall für unsere Twitterdaten wäre die Analyse der Quelle der Tweets, um zum Beispiel herauszufinden, ob es einen Unterschied macht, ob ein Tweet von einem Handy oder von einem Computer verschickt wurde. In diesem Fall kann man die Quelle nicht einfach entfernen, da man sie zur Gruppierung der Tweets braucht. Wenn man wissen möchte, wie sich Tweets im Laufe der Zeit verändern, dann sind die Zeitstempel wichtig. In beiden Fällen würde man aber diese Informationen trotzdem aus dem eigentlichen Text entfernen und sie stattdessen als separate Merkmale speichern, da es sich um Metadaten über die Dokumente handelt.</p>
</section>
<section id="zeichensetzung-und-groszschreibung">
<h3><span class="section-number">10.1.3. </span>Zeichensetzung und Großschreibung<a class="headerlink" href="#zeichensetzung-und-groszschreibung" title="Permalink to this headline">#</a></h3>
<p>Für das Thema von Dokumenten sind die Zeichensetzung und die Groß- und Kleinschreibung von Buchstaben in der Regel irrelevant. Stattdessen führen sie dazu, dass es ungewollte Unterschiede zwischen Worten gibt. Eine Ausnahme von dieser Regel bilden Abkürzungen und Akronyme. Das Akronym <code class="docutils literal notranslate"><span class="pre">U.S.</span></code> der Tweets ist ein perfektes Beispiel hierfür, da es zum Wort <code class="docutils literal notranslate"><span class="pre">us</span></code> werden würde, wenn man einfach alle Zeichen entfernen und alle Buchstaben zu Kleinbuchstaben umwandeln würde. Wir würden also eine ganz andere Bedeutung erhalten. Wenn solche Fälle bekannt sind, sollte man diese manuell adressieren, bevor die Zeichensetzung und Großschreibung entfernt wird. Man könnte zum Beispiel <code class="docutils literal notranslate"><span class="pre">US</span></code> und <code class="docutils literal notranslate"><span class="pre">U.S.</span></code> zu <code class="docutils literal notranslate"><span class="pre">usa</span></code> umwandeln, um zu verhindern, dass es ein Problem gibt.</p>
<p>Wenn wir dies bei den Tweets anwenden, bekommen wir folgende Dokumente:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">string</span>
<span class="n">tweets_lowercase</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">tweet</span> <span class="ow">in</span> <span class="n">tweets_relevant_content</span><span class="p">:</span>
    <span class="n">modified_tweet</span> <span class="o">=</span> <span class="n">tweet</span><span class="o">.</span><span class="n">translate</span><span class="p">(</span><span class="nb">str</span><span class="o">.</span><span class="n">maketrans</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">string</span><span class="o">.</span><span class="n">punctuation</span><span class="p">))</span>
    <span class="n">modified_tweet</span> <span class="o">=</span> <span class="n">modified_tweet</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;US&#39;</span><span class="p">,</span> <span class="s1">&#39;usa&#39;</span><span class="p">)</span>
    <span class="n">modified_tweet</span> <span class="o">=</span> <span class="n">modified_tweet</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
    <span class="n">tweets_lowercase</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">modified_tweet</span><span class="p">)</span>
    
<span class="k">for</span> <span class="n">tweet</span> <span class="ow">in</span> <span class="n">tweets_lowercase</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">wrapper</span><span class="o">.</span><span class="n">wrap</span><span class="p">(</span><span class="n">tweet</span><span class="p">)))</span>
    <span class="nb">print</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>beautiful evening in rochester minnesota vote vote vote

thank you minnesota  i love you

just made my second stop in minnesota for a make america great
again rally we need to elect karinhousley to the usa senate and
we need the strong leadership of tomemmer jason2cd jimhagedornmn
and petestauber in the usa house

congressman bishop is doing a great job he helped pass tax reform
which lowered taxes for everyone nancy pelosi is spending
hundreds of thousands of dollars on his opponent because they
both support a liberal agenda of higher taxes and wasteful
spending

usa stocks widen global lead

statement on national strategy for counterterrorism

working hard thank you

this is now the 7th time the fbi has investigated judge kavanaugh
if we made it 100 it would still not be good enough for the
obstructionist democrats
</pre></div>
</div>
</div>
</div>
</section>
<section id="stoppworter">
<h3><span class="section-number">10.1.4. </span>Stoppwörter<a class="headerlink" href="#stoppworter" title="Permalink to this headline">#</a></h3>
<p>Eine weitere Eigenschaft von Text ist, dass nicht jedes Wort relevant für die Bedeutung ist. Oft werden Wörter nur für die korrekte grammatikalische Struktur benötigt, ohne dass sie die Bedeutung eines Dokuments beeinflussen. Beispiele dafür sind Artikel (der, die, das, den, dem bzw. auf Englisch the, a). Außerdem gibt es noch Wörter, die sehr häufig vorkommen, unabhängig vom Inhalt, zum Beispiel die verschiedenen Formen von sein und haben. Daher besteht ein üblicher Preprocessing-Schritt darin, solche Wörter zu entfernen. Hierzu nutzt man Wortlisten, die es für viele Sprachen als Teil von Textverarbeitungsbibliotheken gibt.</p>
<p>Mit einer englischen Stoppwortliste verändern sich unsere Tweets wie folgt:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">stopwords</span> 
<span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">word_tokenize</span> 

<span class="c1"># this needs only to be run once</span>
<span class="c1"># uncomment this line to download the stopword and punctuation lists</span>
<span class="c1"># import nltk; nltk.download(&#39;stopwords&#39;); nltk.download(&#39;punkt&#39;)</span>

<span class="n">stop_words</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s1">&#39;english&#39;</span><span class="p">))</span> 

<span class="n">tweets_no_stopwords</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">tweet</span> <span class="ow">in</span> <span class="n">tweets_lowercase</span><span class="p">:</span>
    <span class="n">tweet_tokens</span> <span class="o">=</span> <span class="n">word_tokenize</span><span class="p">(</span><span class="n">tweet</span><span class="p">)</span> 
    <span class="n">modified_tweet</span> <span class="o">=</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">tweet_tokens</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">stop_words</span><span class="p">])</span>
    <span class="n">tweets_no_stopwords</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">modified_tweet</span><span class="p">)</span>
    
<span class="k">for</span> <span class="n">tweet</span> <span class="ow">in</span> <span class="n">tweets_no_stopwords</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">wrapper</span><span class="o">.</span><span class="n">wrap</span><span class="p">(</span><span class="n">tweet</span><span class="p">)))</span>
    <span class="nb">print</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>beautiful evening rochester minnesota vote vote vote

thank minnesota love

made second stop minnesota make america great rally need elect
karinhousley usa senate need strong leadership tomemmer jason2cd
jimhagedornmn petestauber usa house

congressman bishop great job helped pass tax reform lowered taxes
everyone nancy pelosi spending hundreds thousands dollars
opponent support liberal agenda higher taxes wasteful spending

usa stocks widen global lead

statement national strategy counterterrorism

working hard thank

7th time fbi investigated judge kavanaugh made 100 would still
good enough obstructionist democrats
</pre></div>
</div>
</div>
</div>
</section>
<section id="stemming-und-lemmatisierung">
<h3><span class="section-number">10.1.5. </span>Stemming und Lemmatisierung<a class="headerlink" href="#stemming-und-lemmatisierung" title="Permalink to this headline">#</a></h3>
<p>Für die gleiche Grundform gibt es oft viele grammatikalische Formen (z.B. Singular und Plural), was zu verschiedenen Schreibweisen führt. Außerdem gibt es verwandte Verben, Adjektive und Nomen. Hinzu kommen Synonyme, also mehrere Wörter mit der gleichen Bedeutung. Für das Text Mining ist es wichtig, zu erkennen, dass es sich eigentlich um die gleiche Bedeutung handelt. Ein Ansatz besteht darin, dass man versucht, alle Wörter auf ein gemeinsames Merkmal abzubilden. Stemming und Lemmatisierung sind zwei Methoden, mit denen man dies erreichen kann.</p>
<p>Beim <em>Stemming</em> werden Wörter auf ihren Stamm reduziert. Aus dem deutschen <code class="docutils literal notranslate"><span class="pre">lachte</span></code> und <code class="docutils literal notranslate"><span class="pre">lachen</span></code> wird <code class="docutils literal notranslate"><span class="pre">lach</span></code>, aus dem englischen <code class="docutils literal notranslate"><span class="pre">spending</span></code> und <code class="docutils literal notranslate"><span class="pre">spends</span></code> wird <code class="docutils literal notranslate"><span class="pre">spend</span></code>. Für das Stemming wird in der Regel ein algorithmischer Ansatz verwendet, zum Beispiel Porters Algorithmus <a class="footnote-reference brackets" href="#porter" id="id1">1</a>. Der Nachteil beim Stemming ist, dass man Wörter nur auf ihren Stamm reduzieren kann. Ähnliche Wörter mit einem anderen Stamm werden nicht vereinheitlicht, zum Beispiel gewinnen und gewann oder good und well.</p>
<p>Bei der <em>Lemmatisierung</em> wird mit Wortlisten gearbeitet. Diese Wortlisten definieren, welche Wörter als Synonyme behandelt werden sollen. Anschließend kann man eines der Synonyme auswählen. Auf diese Weise könnte man zum Beispiel alle Formen von <code class="docutils literal notranslate"><span class="pre">good</span></code> erkennen, insbesondere auch Formen wie <code class="docutils literal notranslate"><span class="pre">well</span></code>, die nicht den gleichen Wortstamm haben. Die Lemmatisierung ist jedoch nur so mächtig, wie es die Wortlisten zulassen.</p>
<p>Wenn man sowohl Lemmatisierung als auch Stemming anwenden möchte, sollte man immer zuerst Lemmatisieren. Der Grund liegt darin, dass die durch das Stemming gefundenen Wortstämme nicht immer Wörter sind, die es in der natürlichen Sprache gibt. Entsprechend gibt es auch keine Einträge in den Wortlisten für das Lemmatisieren. Man reduziert also die Mächtigkeit der Lemmatisierung, wenn man zuerst Stemming anwendet.</p>
<p>Für unsere Tweets verwenden wir zuerst eine Lemmatisierung mit einer englischen Wortliste.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># The following two lines must be run once in each environment</span>
<span class="c1"># This downloads the list from nltk for lemmatization</span>
<span class="c1"># import nltk; nltk.download(&#39;wordnet&#39;)</span>

<span class="kn">from</span> <span class="nn">nltk.stem</span> <span class="kn">import</span> <span class="n">WordNetLemmatizer</span>

<span class="c1"># the wordnet lemmatizer does not only harmonize synonyms but also performs some stemming</span>
<span class="n">lemmatizer</span> <span class="o">=</span> <span class="n">WordNetLemmatizer</span><span class="p">()</span> 

<span class="n">tweets_lemmatization</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">tweet</span> <span class="ow">in</span> <span class="n">tweets_no_stopwords</span><span class="p">:</span>
    <span class="n">tweet_tokens</span> <span class="o">=</span> <span class="n">word_tokenize</span><span class="p">(</span><span class="n">tweet</span><span class="p">)</span> 
    <span class="n">modified_tweet</span> <span class="o">=</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">lemmatizer</span><span class="o">.</span><span class="n">lemmatize</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">tweet_tokens</span><span class="p">])</span>
    <span class="n">tweets_lemmatization</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">modified_tweet</span><span class="p">)</span>
    
<span class="k">for</span> <span class="n">tweet</span> <span class="ow">in</span> <span class="n">tweets_lemmatization</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">wrapper</span><span class="o">.</span><span class="n">wrap</span><span class="p">(</span><span class="n">tweet</span><span class="p">)))</span>
    <span class="nb">print</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>beautiful evening rochester minnesota vote vote vote

thank minnesota love

made second stop minnesota make america great rally need elect
karinhousley usa senate need strong leadership tomemmer jason2cd
jimhagedornmn petestauber usa house

congressman bishop great job helped pas tax reform lowered tax
everyone nancy pelosi spending hundred thousand dollar opponent
support liberal agenda higher tax wasteful spending

usa stock widen global lead

statement national strategy counterterrorism

working hard thank

7th time fbi investigated judge kavanaugh made 100 would still
good enough obstructionist democrat
</pre></div>
</div>
</div>
</div>
<p>Einige Wörter wurden ersetzt, aus <code class="docutils literal notranslate"><span class="pre">stocks</span></code> wurde zum Beispiel <code class="docutils literal notranslate"><span class="pre">stock</span></code>. Mit Stemming erhalten wir folgende Veränderungen:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nltk.stem</span> <span class="kn">import</span> <span class="n">PorterStemmer</span>

<span class="c1"># porter stemming is a linguistic algorithm that provides additional stemming</span>
<span class="n">stemmer</span> <span class="o">=</span> <span class="n">PorterStemmer</span><span class="p">()</span>

<span class="n">tweets_stemming</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">tweet</span> <span class="ow">in</span> <span class="n">tweets_lemmatization</span><span class="p">:</span>
    <span class="n">tweet_tokens</span> <span class="o">=</span> <span class="n">word_tokenize</span><span class="p">(</span><span class="n">tweet</span><span class="p">)</span> 
    <span class="n">modified_tweet</span> <span class="o">=</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">stemmer</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">tweet_tokens</span><span class="p">])</span>
    <span class="n">tweets_stemming</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">modified_tweet</span><span class="p">)</span>
    
<span class="k">for</span> <span class="n">tweet</span> <span class="ow">in</span> <span class="n">tweets_stemming</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">wrapper</span><span class="o">.</span><span class="n">wrap</span><span class="p">(</span><span class="n">tweet</span><span class="p">)))</span>
    <span class="nb">print</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>beauti even rochest minnesota vote vote vote

thank minnesota love

made second stop minnesota make america great ralli need elect
karinhousley usa senat need strong leadership tomemm jason2cd
jimhagedornmn petestaub usa hous

congressman bishop great job help pa tax reform lower tax everyon
nanci pelosi spend hundr thousand dollar oppon support liber
agenda higher tax wast spend

usa stock widen global lead

statement nation strategi counterterror

work hard thank

7th time fbi investig judg kavanaugh made 100 would still good
enough obstructionist democrat
</pre></div>
</div>
</div>
</div>
<p>Viele der Wörter sind jetzt kürzer, unter anderem wurde <code class="docutils literal notranslate"><span class="pre">hundred</span></code> auf <code class="docutils literal notranslate"><span class="pre">hundr</span></code>, <code class="docutils literal notranslate"><span class="pre">wasteful</span></code> auf <code class="docutils literal notranslate"><span class="pre">wast</span></code> und <code class="docutils literal notranslate"><span class="pre">spending</span></code> auf <code class="docutils literal notranslate"><span class="pre">spend</span></code> reduziert. Zwei dieser Beispiele sind keine echten Wörter mehr, da die Wortstämme <code class="docutils literal notranslate"><span class="pre">hundr</span></code> und <code class="docutils literal notranslate"><span class="pre">wast</span></code> nicht in der englischen Sprache vorkommen.</p>
</section>
<section id="visualisierung-des-preprocessings">
<h3><span class="section-number">10.1.6. </span>Visualisierung des Preprocessings<a class="headerlink" href="#visualisierung-des-preprocessings" title="Permalink to this headline">#</a></h3>
<p>Eine einfache Möglichkeit, textuelle Daten zu visualisieren, sind <em>Wortwolken</em> (engl. <em>word clouds</em>). Wortwolken zeigen wichtige Wörter, die in Texten häufig vorkommen. Je häufiger ein Wort vorkommt, desto größer wird es dargestellt. Mithilfe von Wortwolken können wir uns gut die Auswirkungen der Preprocessing-Schritte auf unsere Tweets veranschaulichen.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">wordcloud</span> <span class="kn">import</span> <span class="n">WordCloud</span>

<span class="n">wc_raw</span> <span class="o">=</span> <span class="n">WordCloud</span><span class="p">(</span><span class="n">background_color</span><span class="o">=</span><span class="s2">&quot;white&quot;</span><span class="p">)</span>
<span class="n">wc_raw</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tweets_list</span><span class="p">))</span>

<span class="n">wc_relevant</span> <span class="o">=</span> <span class="n">WordCloud</span><span class="p">(</span><span class="n">background_color</span><span class="o">=</span><span class="s2">&quot;white&quot;</span><span class="p">)</span>
<span class="n">wc_relevant</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tweets_relevant_content</span><span class="p">))</span>

<span class="n">wc_lowercase</span> <span class="o">=</span> <span class="n">WordCloud</span><span class="p">(</span><span class="n">background_color</span><span class="o">=</span><span class="s2">&quot;white&quot;</span><span class="p">)</span>
<span class="n">wc_lowercase</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tweets_lowercase</span><span class="p">))</span>

<span class="n">wc_stopwords</span> <span class="o">=</span> <span class="n">WordCloud</span><span class="p">(</span><span class="n">background_color</span><span class="o">=</span><span class="s2">&quot;white&quot;</span><span class="p">)</span>
<span class="n">wc_stopwords</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tweets_no_stopwords</span><span class="p">))</span>

<span class="n">wc_lemma</span> <span class="o">=</span> <span class="n">WordCloud</span><span class="p">(</span><span class="n">background_color</span><span class="o">=</span><span class="s2">&quot;white&quot;</span><span class="p">)</span>
<span class="n">wc_lemma</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tweets_lemmatization</span><span class="p">))</span>

<span class="n">wc_stemming</span> <span class="o">=</span> <span class="n">WordCloud</span><span class="p">(</span><span class="n">background_color</span><span class="o">=</span><span class="s2">&quot;white&quot;</span><span class="p">)</span>
<span class="n">wc_stemming</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tweets_stemming</span><span class="p">))</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">11</span><span class="p">))</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">wc_raw</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s2">&quot;bilinear&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Kein Preprocessing&#39;</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">wc_relevant</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s2">&quot;bilinear&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Relevanter Inhalt&#39;</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">wc_lowercase</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s2">&quot;bilinear&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Kleinschreibung und keine Satzzeichen&#39;</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">wc_stopwords</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s2">&quot;bilinear&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Stopwords entfernt&#39;</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">wc_lemma</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s2">&quot;bilinear&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Mit Lemmatisierung&#39;</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">wc_stemming</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s2">&quot;bilinear&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Mit Stemming&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_10_13_0.png" src="../_images/kapitel_10_13_0.png" />
</div>
</div>
</section>
<section id="bag-of-words">
<h3><span class="section-number">10.1.7. </span>Bag-of-Words<a class="headerlink" href="#bag-of-words" title="Permalink to this headline">#</a></h3>
<p>Nach dem Preprocessing können wir eine numerische Repräsentation namens <em>Bag-of-Words</em> erstellen. Ein Bag-of-Words ist ähnlich zu einem One-Hot Encoding für Text. Jedes Wort ist ein Merkmal. Der Wert des Merkmals ist die Häufigkeit des Worts im Dokument. Die Worthäufigkeit wird in diesem Kontext auch als <em>Term Frequency</em> (TF) bezeichnet. Hier zeigt sich auch, warum die Harmonisierung sinnvoll ist: Ähnliche Wörter, die zur gleichen Repräsentation harmonisiert wurden, zählen zur Häufigkeit des gleichen Merkmals. Ein Bag-of-Words für unsere Tweets sieht wie folgt aus.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>

<span class="n">vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">()</span>
<span class="n">bag_of_words</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">tweets_stemming</span><span class="p">)</span>
<span class="n">bag_of_words_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">bag_of_words</span><span class="o">.</span><span class="n">todense</span><span class="p">(),</span> <span class="n">columns</span><span class="o">=</span><span class="n">vectorizer</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">())</span>
<span class="n">bag_of_words_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>100</th>
      <th>7th</th>
      <th>agenda</th>
      <th>america</th>
      <th>beauti</th>
      <th>bishop</th>
      <th>congressman</th>
      <th>counterterror</th>
      <th>democrat</th>
      <th>dollar</th>
      <th>...</th>
      <th>thank</th>
      <th>thousand</th>
      <th>time</th>
      <th>tomemm</th>
      <th>usa</th>
      <th>vote</th>
      <th>wast</th>
      <th>widen</th>
      <th>work</th>
      <th>would</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>...</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>8 rows × 70 columns</p>
</div></div></div>
</div>
<p>Den Bag-of-Words kann man als Eingabe für Algorithmen nutzen. Beim Clustering würde man Dokumente, die die gleichen Wörter benutzen, gemeinsam gruppieren, bei der Klassifikation würde man Scores und Klassen basierend auf den Worthäufigkeiten berechnen.</p>
</section>
<section id="inverse-document-frequency">
<h3><span class="section-number">10.1.8. </span>Inverse Document Frequency<a class="headerlink" href="#inverse-document-frequency" title="Permalink to this headline">#</a></h3>
<p>Eine beliebte Erweiterung des Bag-of-Words ist die <em>Inverse Document Frequency</em> (IDF). Hinter der IDF steckt die Idee, dass man die Wörter nach ihrer “Einzigartigkeit” gewichten sollte. Wenn ein Wort in nur wenigen Dokumenten vorkommt, könnte es sehr spezifisch sein und sollte daher einen größeren Einfluss haben. Wenn ein Wort in sehr vielen Dokumenten vorkommt, ist es eher unspezifisch und sollte daher nur geringen Einfluss haben. Diese Idee ist ähnlich zur Entfernung von Stoppwörtern, nur dass mit Gewichten gearbeitet wird und keine Begriffe komplett entfernt werden. Die IDF ist definiert als</p>
<div class="math notranslate nohighlight">
\[IDF_t = \log\frac{N}{D_t},\]</div>
<p>wobei <span class="math notranslate nohighlight">\(t\)</span> ein Wort (Term) ist, <span class="math notranslate nohighlight">\(N\)</span> die Anzahl der Dokumente im Korpus und <span class="math notranslate nohighlight">\(D_t\)</span> die Anzahl der Dokumente, die <span class="math notranslate nohighlight">\(t\)</span> enthält. Die TF-IDF kombiniert die Term Frequency mit den Gewichten der IDF und kann anstatt der TF im Bag-of-Words benutzt werden. TF-IDF ist für ein Wort <span class="math notranslate nohighlight">\(t\)</span> definiert als</p>
<div class="math notranslate nohighlight">
\[TF\text{-}IDF_t = TF_t \cdot IDF_t.\]</div>
<p>Mit unseren Tweets erhalten wir einen Bag-of-Words mit folgenden TF-IDF-Werten:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">bag_of_words_df</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">col</span><span class="p">:</span> <span class="n">col</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">col</span><span class="p">)</span><span class="o">/</span><span class="n">col</span><span class="o">.</span><span class="n">gt</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>100</th>
      <th>7th</th>
      <th>agenda</th>
      <th>america</th>
      <th>beauti</th>
      <th>bishop</th>
      <th>congressman</th>
      <th>counterterror</th>
      <th>democrat</th>
      <th>dollar</th>
      <th>...</th>
      <th>thank</th>
      <th>thousand</th>
      <th>time</th>
      <th>tomemm</th>
      <th>usa</th>
      <th>vote</th>
      <th>wast</th>
      <th>widen</th>
      <th>work</th>
      <th>would</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>2.079442</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>6.238325</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>1.386294</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>2.079442</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>2.079442</td>
      <td>2.772589</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>2.079442</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>2.079442</td>
      <td>2.079442</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>2.079442</td>
      <td>...</td>
      <td>0.000000</td>
      <td>2.079442</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>2.079442</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.386294</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>2.079442</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>2.079442</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>1.386294</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>2.079442</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>7</th>
      <td>2.079442</td>
      <td>2.079442</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>2.079442</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>2.079442</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>2.079442</td>
    </tr>
  </tbody>
</table>
<p>8 rows × 70 columns</p>
</div></div></div>
</div>
<blockquote>
<div><p><strong>Bemerkung:</strong></p>
<p>Die IDF wird oft mit einer Glättung als <span class="math notranslate nohighlight">\(IDF_t = \log\frac{N}{D_t+1}\)</span> definiert. Hierdurch werden undefinierte Werte durch einen Logarithmus von 0 vermieden, wenn ein Wort nicht in einem Korpus auftaucht. Es gibt noch andere ähnliche Glättungen. Diese Glättung hat in der Regel keinen Einfluss auf die Ergebnisse, verhindert jedoch oft Schwierigkeiten, indem die numerische Stabilität erhöht wird.</p>
</div></blockquote>
</section>
<section id="jenseits-des-bag-of-words">
<h3><span class="section-number">10.1.9. </span>Jenseits des Bag-of-Words<a class="headerlink" href="#jenseits-des-bag-of-words" title="Permalink to this headline">#</a></h3>
<p>Der Bag-of-Words ist eine gute Repräsentation, die sehr gut auch bei großen Datenmengen skaliert. Mit dieser relativ einfachen Repräsentation von Text sind jedoch auch diverse Nachteile verbunden. Zum einen werden alle strukturellen Aspekte von Text ignoriert, zum Beispiel die Grammatik. Zum anderen werden Ähnlichkeiten nicht berücksichtigt. Die Worte <code class="docutils literal notranslate"><span class="pre">dollar</span></code> und <code class="docutils literal notranslate"><span class="pre">euro</span></code> sind, basierend auf dem Bag-of-Words, genauso verschieden voneinander wie <code class="docutils literal notranslate"><span class="pre">elefant</span></code> und <code class="docutils literal notranslate"><span class="pre">computer</span></code>. Hierdurch ist es schwierig, einen abstrakteren Kontext zu lernen, zum Beispiel, dass etwas mit einer Währung zusammenhängt. Aus diesem Grund sind Bag-of-Words im modernen Text Mining eher als Einstieg zu verstehen, der für einfache Anwendungsfälle genügt, aber bei komplexen Anwendungsfällen oft nicht ausreichend ist. Eine relativ einfache Erweiterung sind <span class="math notranslate nohighlight">\(n\)</span>-grams, bei denen nicht individuelle Wörter, sondern Sequenzen aus <span class="math notranslate nohighlight">\(n\)</span> Wörtern als Merkmale verwendet werden. Hierdurch wird der Kontext zu einem gewissen Grad erfasst. Da es aber sehr viele Kombinationen von Wörtern gibt, skalieren <span class="math notranslate nohighlight">\(n\)</span>-grams nur sehr schlecht.</p>
<p>Neuere Text-Mining-Verfahren basieren daher auf neuronalen Netzen. Zum einen gibt es <em>Worteinbettungen</em> (engl. <em>word embeddings</em>), bei denen Wörter als Vektoren in einem hochdimensionalen Raum dargestellt werden. Je näher sich zwei Wörter in dieser Worteinbettung sind, desto ähnlicher sind die Wörter. Worteinbettungen selbst kann man ähnlich zu den Wortlisten, die uns oben begegnet sind, häufig direkt herunterladen und anwenden. Alternativ kann man für einen Text eigene Einbettungen mit einem einfachen neuronalen Netz lernen. Die neuesten Text-Mining-Verfahren basieren auf tiefen neuronalen Netzen mit einer sogenannten Transformer-Architektur. Derartige neuronale Netze betrachten nicht nur einzelne Wörter, sondern den kompletten Kontext eines Textes. Wie sich der Kontext von Wörtern zusammensetzt, wird aus riesigen Datenmengen gelernt, um dies in Anwendungen ausnutzen zu können.</p>
</section>
</section>
<section id="herausforderungen-des-text-mining">
<h2><span class="section-number">10.2. </span>Herausforderungen des Text Mining<a class="headerlink" href="#herausforderungen-des-text-mining" title="Permalink to this headline">#</a></h2>
<p>Auch wenn es mittlerweile gute Repräsentationen für Text gibt, sind viele Herausforderungen des Text Mining immer noch ungelöst und manche auch gar nicht lösbar.</p>
<section id="dimensionalitat">
<h3><span class="section-number">10.2.1. </span>Dimensionalität<a class="headerlink" href="#dimensionalitat" title="Permalink to this headline">#</a></h3>
<p>Das erste Problem ist die <em>Dimensionalität</em>. In unserem Beispiel haben wir lediglich acht Tweets, aber trotzdem noch 70 verschiedene Wörter nach dem Preprocessing. In einem längeren Text gibt es häufig Tausende von Wörtern. Es gibt also sehr viele Merkmale. Hinzu kommt, dass der Korpus eventuell auch sehr viele Dokumente enthalten kann. Täglich gibt es zum Beispiel über 100 Millionen Tweets, die im Rahmen einer umfassenden Analyse von sozialen Netzwerken betrachtet werden müssten.</p>
<p>Die Kombination aus vielen Merkmalen und vielen Instanzen führt dazu, dass man für Text Mining häufig hohe Anforderungen an die Analyseumgebung stellt. Große Text-Mining-Projekte, insbesondere solche, die auf neuronalen Netzen basieren, benötigen daher spezielle Hardware. Dies ist auch ein Grund, warum ein einfacher Ansatz wie der Bag-of-Words immer noch hoch relevant ist: Mit einem Bag-of-Words und einem relativ einfachen Algorithmus wie Multinomial Naive Bayes kann man häufig auch auf einfacher Hardware bereits gute Ergebnisse erreichen.</p>
</section>
<section id="mehrdeutigkeiten">
<h3><span class="section-number">10.2.2. </span>Mehrdeutigkeiten<a class="headerlink" href="#mehrdeutigkeiten" title="Permalink to this headline">#</a></h3>
<p>Das zweite Problem ist die <em>Mehrdeutigkeit</em> der natürlichen Sprache. Teilweise ist dieses Problem nicht lösbar, da man häufig den Kontext benötigt, um den Inhalt zu verstehen. Der Kontext selbst besteht aber nicht nur aus dem Text, sondern auch aus den Aspekten rund um ein Dokument. Innerhalb dieses Buches ist der Kontext des Wortes “Lernen” zum Beispiel ein anderer als in einem Buch über Didaktik. Hier ist ein Beispiel für einen Satz, den man nur im Kontext versteht:</p>
<ul class="simple">
<li><p>Ist das sicher genug? (Ist diese Erkenntnis gesichert?)</p></li>
<li><p>Ist das sicher genug? (Ist die Gefahr eines Unfalls gering?)</p></li>
</ul>
<p>Es gibt viele Beispiele für derartige Probleme, die unweigerlich zu Rauschen beim Text Mining führen.</p>
<p>Häufig liegen diese Probleme schon direkt bei den Wörtern, die wir verwenden: <em>Homonyme</em> sind Wörter mit mehreren Bedeutungen. Das Wort “erfassen” kann zum Beispiel heißen, dass man etwas aufnimmt, aber auch dass man von einem Fahrzeug überfahren wurde. Um Homonyme korrekt zu verarbeiten, muss man sie in ihrem Kontext interpretieren, was insbesondere beim Bag-of-Words ein Problem ist.</p>
</section>
<section id="weitere-probleme">
<h3><span class="section-number">10.2.3. </span>Weitere Probleme<a class="headerlink" href="#weitere-probleme" title="Permalink to this headline">#</a></h3>
<p>Es gibt noch weitere Probleme, die wir hier aber nicht im Detail diskutieren:</p>
<ul class="simple">
<li><p>Rechtschreibfehler führen zu unbekannten Wörtern und lassen sich häufig nicht automatisch korrigieren.</p></li>
<li><p>Sprache entwickelt sich weiter, insbesondere in Form von neuen Wörtern und Jugendsprache, aber auch durch domänenspezifische Fachbegriffe.</p></li>
<li><p>Beim Preprocessing kann es leicht passieren, Aspekte zu übersehen, zum Beispiel durch mangelhafte Wortlisten.</p></li>
<li><p>Das Lesen von Text selbst ist häufig bereits ein Problem, insbesondere wenn verschiedene Codierungen (ASCII, UNICODE) und Zeichensätze (Chinesisch, Japanisch, Koreanisch, Arabisch, Kyrillisch, Lateinisch, …) verwendet werden.</p></li>
</ul>
<p>Das heißt auch, dass man bei Text-Mining-Anwendungen in der Regel kein perfektes Ergebnis bekommt, sondern nur immer weitere Probleme lösen kann. Deshalb ist es bei Text-Mining-Projekten besonders wichtig, klare Ziele zu formulieren und hierdurch sicherzustellen, dass ein Projekt auch wirklich beendet wird und nicht immer weitere Sonderfälle betrachtet werden.</p>
</section>
</section>
<section id="ubung">
<h2><span class="section-number">10.3. </span>Übung<a class="headerlink" href="#ubung" title="Permalink to this headline">#</a></h2>
<p>In dieser Übung wollen wir das Text Mining mit einem größeren Beispiel vertiefen. Hierzu verwenden wir eine Erweiterung der oben genannten Tweets: Statt nur acht Tweets betrachten wir den vollständigen Korpus der Tweets von Donald Trump aus dem Jahr 2017 <a class="footnote-reference brackets" href="#tweets" id="id2">2</a>.</p>
<section id="wortwolke-ohne-preprocessing">
<h3><span class="section-number">10.3.1. </span>Wortwolke ohne Preprocessing<a class="headerlink" href="#wortwolke-ohne-preprocessing" title="Permalink to this headline">#</a></h3>
<p>Laden Sie die Daten. Erstellen Sie eine Wortwolke ohne jegliches Preprocessing. Welche Probleme gibt es? Was erkennt man eventuell trotzdem bereits?</p>
</section>
<section id="wortwolke-mit-preprocessing">
<h3><span class="section-number">10.3.2. </span>Wortwolke mit Preprocessing<a class="headerlink" href="#wortwolke-mit-preprocessing" title="Permalink to this headline">#</a></h3>
<p>Wenden Sie die Preprocessing-Schritte, die wir diskutiert haben, an und erstellen Sie eine neue Wortwolke.</p>
</section>
<section id="tf-idf">
<h3><span class="section-number">10.3.3. </span>TF-IDF<a class="headerlink" href="#tf-idf" title="Permalink to this headline">#</a></h3>
<p>Die Wortwolken benutzen üblicherweise die Worthäufigkeiten. Berechnen Sie die TF-IDF und erstellen Sie neue Wortwolken basierend auf den auf diese Art gewichteten Worthäufigkeiten. Wie verändert sich das Ergebnis?</p>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="porter"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p><a class="reference external" href="https://doi.org/10.1108/eb046814">https://doi.org/10.1108/eb046814</a></p>
</dd>
<dt class="label" id="tweets"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p><a class="reference external" href="https://data-science-crashkurs.de/exercises/data/trump-tweets-2017.txt">https://data-science-crashkurs.de/exercises/data/trump-tweets-2017.txt</a></p>
</dd>
</dl>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./chapters"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="kapitel_09.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">9. </span>Zeitreihenanalyse</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="kapitel_11.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">11. </span>Statistik</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Steffen Herbold<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>