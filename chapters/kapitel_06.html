
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>6. Clusteranalyse &#8212; ARBEITSVERSION - Data Science Crashkurs - Eine interaktive und praktische Einführung - Bitte lesen sie das Vorwort für weitere Informationen</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="7. Klassifikation" href="kapitel_07.html" />
    <link rel="prev" title="5. Assoziationsregeln" href="kapitel_05.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">ARBEITSVERSION - Data Science Crashkurs - Eine interaktive und praktische Einführung - Bitte lesen sie das Vorwort für weitere Informationen</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="vorwort.html">
   WICHTIG: Arbeitsversion
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Kapitel
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="kapitel_01.html">
   1. Big Data und Data Science
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kapitel_02.html">
   2. Der Prozess von Data-Science-Projekten
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kapitel_03.html">
   3. Allgemeines zur Datenanalyse
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kapitel_04.html">
   4. Erkunden der Daten
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kapitel_05.html">
   5. Assoziationsregeln
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   6. Clusteranalyse
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kapitel_07.html">
   7. Klassifikation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kapitel_08.html">
   8. Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kapitel_09.html">
   9. Zeitreihenanalyse
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kapitel_10.html">
   10. Textmining
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kapitel_11.html">
   11. Statistik
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kapitel_12.html">
   12. Big Data Processing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kapitel_13.html">
   13. Weiterführende Konzepte
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Appendix
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="howto.html">
   Selber Ausführen
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notations.html">
   Notationen
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/chapters/kapitel_06.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/sherbold/einfuehrung-in-data-science"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/sherbold/einfuehrung-in-data-science/main?urlpath=tree/content/chapters/kapitel_06.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ahnlichkeitsmasze">
   6.1. Ähnlichkeitsmaße
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#stadte-und-hauser">
   6.2. Städte und Häuser
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#k-means-algorithmus">
   6.3.
   <span class="math notranslate nohighlight">
    \(k\)
   </span>
   -Means-Algorithmus
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#der-algorithmus">
     6.3.1. Der Algorithmus
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bestimmen-von-k">
     6.3.2. Bestimmen von
     <span class="math notranslate nohighlight">
      \(k\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#probleme-des-k-means-algorithmus">
     6.3.3. Probleme des
     <span class="math notranslate nohighlight">
      \(k\)
     </span>
     -Means-Algorithmus
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#em-clustering">
   6.4. EM-Clustering
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     6.4.1. Der Algorithmus
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id3">
     6.4.2. Bestimmen von
     <span class="math notranslate nohighlight">
      \(k\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#probleme-des-em-clustering">
     6.4.3. Probleme des EM-Clustering
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dbscan">
   6.5. DBSCAN
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id5">
     6.5.1. Der Algorithmus
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bestimmen-von-epsilon-und-minpts">
     6.5.2. Bestimmen von
     <span class="math notranslate nohighlight">
      \(\epsilon\)
     </span>
     und
     <span class="math notranslate nohighlight">
      \(minPts\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#probleme-bei-dbscan">
     6.5.3. Probleme bei DBSCAN
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#single-linkage-clustering">
   6.6. Single Linkage Clustering
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#der-slink-algorithmus">
     6.6.1. Der SLINK-Algorithmus
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dendrogramme">
     6.6.2. Dendrogramme
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#probleme-bei-slink">
     6.6.3. Probleme bei SLINK
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#vergleich-der-algorithmen">
   6.7. Vergleich der Algorithmen
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#clusterformen">
     6.7.1. Clusterformen
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#anzahl-der-cluster">
     6.7.2. Anzahl der Cluster
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#ausfuhrungszeit">
     6.7.3. Ausführungszeit
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#interpretierbarkeit-und-darstellung">
     6.7.4. Interpretierbarkeit und Darstellung
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#kategorische-merkmale">
     6.7.5. Kategorische Merkmale
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#fehlende-merkmale">
     6.7.6. Fehlende Merkmale
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#korrelierte-merkmale">
     6.7.7. Korrelierte Merkmale
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#zusammenfassung-des-vergleichs">
     6.7.8. Zusammenfassung des Vergleichs
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ubung">
   6.8. Übung
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id7">
     6.8.1.
     <span class="math notranslate nohighlight">
      \(k\)
     </span>
     -Means-Algorithmus
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id8">
     6.8.2. EM Clustering
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id9">
     6.8.3. DBSCAN
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#slink">
     6.8.4. SLINK
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#vergleichen-sie-die-ergebnisse">
     6.8.5. Vergleichen Sie die Ergebnisse
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="clusteranalyse">
<h1><span class="section-number">6. </span>Clusteranalyse<a class="headerlink" href="#clusteranalyse" title="Permalink to this headline">¶</a></h1>
<p>Bei der Clusteranalyse, häufig auch einfach Clustern oder Clustering genannt, sucht man Gruppen von verwandten Objekten in einer Menge von Instanzen. Diese Gruppen nennt man auch Cluster. Betrachten wir das Beispiel in <a class="reference internal" href="#fig-clust-example"><span class="std std-numref">Fig. 6.1</span></a>. Auf der linken Seite sehen wir verschiedene Emoticons, unsere Objekte. Beim Clustern werden die Objekte jetzt in Gruppen unterteilt, in diesem Fall in zwei Gruppen: die glücklichen Emoticons und die traurigen Emoticons. Die Gruppen werden basierend auf den Merkmalen der Objekte bestimmt, andere Informationen stehen nicht zur Verfügung. Um die Trennung in glücklich und traurig zu erreichen, müssen diese Emotionen also von den Merkmalen repräsentiert werden. Wenn die Merkmale etwas anderes beschreiben würden, zum Beispiel die Farbe, würde das Ergebnis des Clusterns ein anderes sein: eine Trennung in gelbe und orange Emoticons. Wenn die Merkmale die Menge der Haare im Gesicht der Emoticons beschreiben würden, dann würde man die Emoticons ohne Bart von denen mit einem Schnauzer trennen. Auch wenn dies auf den ersten Blick offensichtlich erscheint, ist dieser Punkt extrem wichtig: Wenn die Merkmale nicht zur gewünschten Gruppierung passen, finden wir vielleicht trotzdem Gruppen, doch diese sind möglicherweise nicht sinnvoll für unseren Anwendungsfall.</p>
<div class="figure align-default" id="fig-clust-example">
<a class="reference internal image-reference" href="../_images/clustering_general.png"><img alt="../_images/clustering_general.png" src="../_images/clustering_general.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">Fig. 6.1 </span><span class="caption-text">Gruppieren von Emoticons als Beispiel für das Clustern</span><a class="headerlink" href="#fig-clust-example" title="Permalink to this image">¶</a></p>
</div>
<p>Etwas abstrakter können wir Clustern wie in <a class="reference internal" href="#fig-clust-abstract"><span class="std std-numref">Fig. 6.2</span></a> beschreiben. Wir haben Objekte, für die wir mithilfe von einem Clusteralgorithmus eine sinnvolle Gruppierung suchen. Ohne die gefundenen Gruppen manuell zu analysieren, weiß man nicht, was die Gruppen repräsentieren und ob die Beziehung zwischen den Objekten wirklich sinnvoll ist.</p>
<div class="figure align-default" id="fig-clust-abstract">
<a class="reference internal image-reference" href="../_images/abstract_clustering_german.png"><img alt="../_images/abstract_clustering_german.png" src="../_images/abstract_clustering_german.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">Fig. 6.2 </span><span class="caption-text">Konzept des Clusterings</span><a class="headerlink" href="#fig-clust-abstract" title="Permalink to this image">¶</a></p>
</div>
<p>Formal haben wir eine Menge von Objekten <span class="math notranslate nohighlight">\(O = \{object_1, object_2, ...\}\)</span>, die möglicherweise unendlich viele Elemente enthält. Außerdem haben wir eine Repräsentation der Objekte als Instanzen im Merkmalsraum <span class="math notranslate nohighlight">\(\mathcal{F} = \{\phi(o): o \in O\}\)</span>. Beim Clustern betrachtet man in der Regel numerische Merkmale, das heißt, jede Instanz ist ein reellwertiger Vektor und es gilt also <span class="math notranslate nohighlight">\(\mathcal{F} \subseteq \mathbb{R}^d\)</span>. Das Gruppieren der Objekte wird beschrieben durch eine Abbildung <span class="math notranslate nohighlight">\(c: \mathcal{F} \to G\)</span>, wobei <span class="math notranslate nohighlight">\(G =\{1, ..., k\}\)</span> die Cluster sind und <span class="math notranslate nohighlight">\(k \in \mathbb{N}\)</span> die Anzahl der Cluster.</p>
<div class="section" id="ahnlichkeitsmasze">
<h2><span class="section-number">6.1. </span>Ähnlichkeitsmaße<a class="headerlink" href="#ahnlichkeitsmasze" title="Permalink to this headline">¶</a></h2>
<p>Die Grundvoraussetzung, um ähnliche Objekte gruppieren zu können, ist, dass man ihre Ähnlichkeit messen kann. Beim Clustern ist der Ansatz hierfür, die Ähnlichkeit über den <em>Abstand</em> im Merkmalsraum zu messen. Je näher die Instanzen von Objekten einander sind, desto ähnlicher sind sie. Je weiter entfernt sie voneinander sind, desto verschiedener. Es gibt verschiedene Metriken, mit denen man Distanzen messen kann. In der Regel wird eine der folgenden drei Metriken verwendet.</p>
<p>Die mit großem Abstand üblichste Metrik ist der <em>euklidische Abstand</em>, den wir uns geometrisch als die direkte Verbindung zwischen zwei Punkten vorstellen können (<a class="reference internal" href="#fig-euclidean"><span class="std std-numref">Fig. 6.3</span></a>). Der euklidische Abstand wird über die euklidische Norm <span class="math notranslate nohighlight">\(||\cdot||_2\)</span> definiert als</p>
<div class="math notranslate nohighlight">
\[d(x,y) = ||y-x||_2 = \sqrt{(y_1-x_1)^2+...+(y_n-x_n)^2}.\]</div>
<div class="figure align-default" id="fig-euclidean">
<a class="reference internal image-reference" href="../_images/euclidean.png"><img alt="../_images/euclidean.png" src="../_images/euclidean.png" style="width: 200px;" /></a>
<p class="caption"><span class="caption-number">Fig. 6.3 </span><span class="caption-text">Die euklidische Distanz als direkte Verbindung zweier Punkte</span><a class="headerlink" href="#fig-euclidean" title="Permalink to this image">¶</a></p>
</div>
<p>Ein weiteres verbreitetes Distanzmaß ist die <em>Manhattan-Distanz</em>. Der Ursprung des Namens ist darin begründet, dass diese Metrik ähnlich ist zu der Distanz, die man in Manhattan gehen muss, wenn man sich in den gitterartig angelegten Straßen bewegt. Da diese Straßen zum Großteil achsenparallel sind, kann man nicht diagonal gehen (<a class="reference internal" href="#fig-manhatten"><span class="std std-numref">Fig. 6.4</span></a>). Die Manhattan-Distanz wird über die Manhattan-Norm <span class="math notranslate nohighlight">\(||\cdot||_1\)</span> definiert als</p>
<div class="math notranslate nohighlight">
\[d(x,y) = ||y-x||_1 = |y_1-x_1|+...+|y_n-x_n|.\]</div>
<p>Der Einsatz der Manhattan-Distanz ist zum Beispiel bei sehr hochdimensionalen Daten sinnvoll, also bei Daten mit vielen Merkmalen. Hier gibt es die Tendenz bei der euklidischen Norm, dass alle Abstände ähnlich werden können. Außerdem ist die Manhattan-Norm robuster gegen Ausreißer bei einzelnen Merkmalen, da die Werte der Merkmale nicht vor dem Aufsummieren quadriert werden.</p>
<div class="figure align-default" id="fig-manhatten">
<a class="reference internal image-reference" href="../_images/manhatten.png"><img alt="../_images/manhatten.png" src="../_images/manhatten.png" style="width: 200px;" /></a>
<p class="caption"><span class="caption-number">Fig. 6.4 </span><span class="caption-text">Die Manhattan-Distanz als Verbindung zweier Punkte, ohne sich diagonal bewegen zu können.</span><a class="headerlink" href="#fig-manhatten" title="Permalink to this image">¶</a></p>
</div>
<p>Der <em>Chebyshev-Abstand</em> ist auch als Maximumsmetrik bekannt und misst den maximalen Abstand in eine beliebige Richtung (<a class="reference internal" href="#fig-chebyshev"><span class="std std-numref">Fig. 6.5</span></a>). Der Chebyshev-Abstand wird über die Maximumsnorm <span class="math notranslate nohighlight">\(||\cdot||_\infty\)</span> definiert als</p>
<div class="math notranslate nohighlight">
\[d(x,y) = ||y-x||_\infty = \max_{i=1, ..., n} |y_i-x_i|.\]</div>
<p>Man sollte den Chebyshev-Abstand benutzen, wenn lediglich der maximale Abstand, den man bei den Merkmalen beobachtet, wichtig ist, nicht jedoch wie viele Merkmale sich unterscheiden.</p>
<div class="figure align-default" id="fig-chebyshev">
<a class="reference internal image-reference" href="../_images/chebyshev.png"><img alt="../_images/chebyshev.png" src="../_images/chebyshev.png" style="width: 200px;" /></a>
<p class="caption"><span class="caption-number">Fig. 6.5 </span><span class="caption-text">Der Chebyshev-Abstand als maximale Distanz in eine beliebige Richtung. Dies entspricht der Anzahl der Schritte, die ein König beim Schach bräuchte, um ein Feld zu erreichen.</span><a class="headerlink" href="#fig-chebyshev" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="stadte-und-hauser">
<h2><span class="section-number">6.2. </span>Städte und Häuser<a class="headerlink" href="#stadte-und-hauser" title="Permalink to this headline">¶</a></h2>
<p>Wir benutzen eine Analogie, um die Konzepte der Clusteralgorithmen zu erklären: Unsere Objekte sind Häuser und unsere Cluster sind Städte. Das Schöne an dieser Analogie ist, dass sie intuitiv plausibel ist. Man muss nicht wissen, zu welcher Stadt ein Haus gehört, um zu erkennen, welche weiteren Häuser zu dieser Stadt gehören. Man muss auch nicht wissen, wie viele Städte es gibt, um das zu erkennen. Es reicht, wenn man die Position der Häuser betrachtet. Das ist genau das Problem, das wir mit Clusteralgorithmen lösen wollen: Welche Objekte gehören zum gleichen Cluster und wie viele Cluster gibt es?</p>
</div>
<div class="section" id="k-means-algorithmus">
<h2><span class="section-number">6.3. </span><span class="math notranslate nohighlight">\(k\)</span>-Means-Algorithmus<a class="headerlink" href="#k-means-algorithmus" title="Permalink to this headline">¶</a></h2>
<p>Eine Variante, um Städte zu definieren, könnte mithilfe der Rathäuser erfolgen. Man könnte sagen, dass jedes Haus zu der Stadt gehört, deren Rathaus am nächsten liegt. Diese Idee ist die Essenz des <span class="math notranslate nohighlight">\(k\)</span>-Means-Algorithmus. Man beschreibt Cluster durch ihr <em>Zentrum</em>. Dieses Zentrum nennt man auch <em>Centroid</em>, weshalb es sich bei <span class="math notranslate nohighlight">\(k\)</span>-Means auch um einen Vertreter der <em>centroidbasierten Clusteralgorithmen</em> handelt.</p>
<p>Wenn man wissen möchte, zu welchem Cluster eine Instanz gehört, muss man lediglich herausfinden, welcher Centroid am nächsten ist. Das <span class="math notranslate nohighlight">\(k\)</span> im Namen des Algorithmus steht für die Anzahl der Cluster. Formal haben wir die Centroids <span class="math notranslate nohighlight">\(C_1, ..., C_k\)</span> und eine Metrik <span class="math notranslate nohighlight">\(d\)</span>, um den Abstand zu messen. Wir können für jede Instanz <span class="math notranslate nohighlight">\(x \in \mathcal{F}\)</span> estimmen, zu welchem Cluster sie gehört, indem wir das Minimum bestimmen:</p>
<div class="math notranslate nohighlight">
\[c(x) = argmin_{i=1,...,k} d(x, C_i)\]</div>
<p>bestimmen.</p>
<p>Das folgende Beispiel zeigt, wie wir Daten in <span class="math notranslate nohighlight">\(k=4\)</span> Cluster unterteilen. Die Centroids sind als große graue Punkte dargestellt, die anderen Farben zeigen die Clusterzugehörigkeit.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># we need matplotlib for plotting</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="c1"># we use sklearn to generate data and for the clustering</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>

<span class="c1"># generate sample data, the _ means that we ignore the second return value</span>
<span class="n">X</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">center_box</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                  <span class="n">cluster_std</span><span class="o">=</span><span class="mf">0.04</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># we fit a k-means model with four clusters</span>
<span class="c1"># then we predict for each point to which cluster it belong</span>
<span class="c1"># finally, we determine the location of the cluster centers</span>
<span class="c1"># n_init, init and random_state should usually not be set. </span>
<span class="c1"># we only use these parameters to make sure we have results</span>
<span class="c1"># that we can re-use later to demonstrate how the algorithm works</span>
<span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s1">&#39;random&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">kmeans</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">y_kmeans</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">centers</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">cluster_centers_</span>

<span class="c1"># now we plot the data and the clustering results</span>
<span class="n">f</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Daten (ohne Clustering)&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Ergebnis des $k$-Means-Algorithmus, $k=4$&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y_kmeans</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">centers</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">centers</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_06_1_0.png" src="../_images/kapitel_06_1_0.png" />
</div>
</div>
<p>Diese Darstellung der Cluster ist einfach, effektive und intuitiv. Der Nachteil dieser Darstellung von Clustern ist, dass dies nur mit zwei Merkmalen funktioniert. Wenn man mehr als zwei Merkmale hat, muss man entweder paarweise Scatterplots nutzen oder eine Technik wie die PCA einsetzen, um eine zweidimensionale Darstellung zu bekommen (siehe <a class="reference internal" href="kapitel_04.html"><span class="doc std std-doc">Kapitel 4</span></a>).</p>
<div class="section" id="der-algorithmus">
<h3><span class="section-number">6.3.1. </span>Der Algorithmus<a class="headerlink" href="#der-algorithmus" title="Permalink to this headline">¶</a></h3>
<p>Ziel des Clusteralgorithmus ist es, die Positionen der Centroids zu bestimmen. Wenn wir uns wieder eine Stadt vorstellen, könnte man argumentieren, dass der Platz, an dem ein Rathaus gebaut werden soll, so gewählt werden sollte, dass der mittlere Abstand der Häuser zum Rathaus minimiert wird. Dann haben die Bewohner - im Mittel - den kürzesten Weg zum Rathaus. Jetzt stellen Sie sich vor, dass ein Neubaugebiet entsteht, außerdem werden an anderer Stelle ein paar alte Häuser abgerissen. Hierdurch ist der Ort des Rathauses nicht mehr optimal und idealerweise wird es neu gebaut, sodass der Abstand wieder minimiert wird. Hier ist die Analogie problematisch: Der Neubau eines Rathauses ist teuer, einen Centroid zu verschieben jedoch sehr einfach. Diese Idee ist also genau das, was der Algorithmus macht.</p>
<p>Seien <span class="math notranslate nohighlight">\(X \subseteq \mathcal{F}\)</span> unsere Daten. Der <span class="math notranslate nohighlight">\(k\)</span>-Means-Algorithmus bestimmt die Cluster bzw. die Centroids durch folgenden iterativen Algorithmus:</p>
<ol class="simple">
<li><p>Wähle die Startposition der Centroids <span class="math notranslate nohighlight">\(C_1, ..., C_k \in \mathcal{F}\)</span>.</p></li>
<li><p>Bestimme die Cluster <span class="math notranslate nohighlight">\(X_i = \{x \in X: c(x) = i\}\)</span> für <span class="math notranslate nohighlight">\(i=1,...,k\)</span>.</p></li>
<li><p>Verschiebe die Centroids, sodass der neue Ort des Centroids dem arithmetischen Mittel der Instanzen des Clusters entspricht: <span class="math notranslate nohighlight">\(C_i = \frac{1}{|X_i|} \sum_{x \in X_i} x\)</span>.</p></li>
<li><p>Wiederhole die Schritte 2 und 3, bis</p>
<ul class="simple">
<li><p>das Ergebnis konvergiert, also sich die Cluster <span class="math notranslate nohighlight">\(C_1, ..., C_k\)</span> nicht mehr verändern, oder</p></li>
<li><p>eine vorher festgelegte Höchstanzahl an Iterationen erreicht ist.</p></li>
</ul>
</li>
</ol>
<p>Auch wenn dieser Algorithmus relativ abstrakt klingt, wird schnell klar, wie er funktioniert, wenn wir uns die Schritte veranschaulichen.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">f</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="c1"># run the algorithm with k=1,...,4 iterations to demonstrate how it converges</span>
<span class="k">for</span> <span class="nb">iter</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="nb">iter</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s1">&#39;random&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
    <span class="n">kmeans</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">y_kmeans</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">centers</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">cluster_centers_</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="nb">min</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">iter</span><span class="o">-</span><span class="mi">2</span><span class="p">)),</span> <span class="p">(</span><span class="nb">iter</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">%</span><span class="k">2</span>]
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Iteration </span><span class="si">%i</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="nb">iter</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y_kmeans</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">centers</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">centers</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_06_3_0.png" src="../_images/kapitel_06_3_0.png" />
</div>
</div>
<p>Im Detail passiert das Folgende:</p>
<ul class="simple">
<li><p>Iteration 1: Zuerst sehen wir den Startzustand mit zufällig gewählten Centroids und daher auch einer relativ zufälligen Aufteilung der Instanzen in Cluster. Ein Centroid befindet sich zwischen den Instanzen oben rechts und unten links. Dieser Centroid liegt am nächsten zu den gelb markierten Instanzen oben rechts sowie einer einzelnen Instanz. Ein weiterer Centroid liegt mittig in der unteren Hälfte der Daten und hat die blauen Instanzen zugewiesen bekommen. Außerdem gibt es noch zwei Centroids, die sich recht nah beieinander in der linken unteren Hälfte der Daten befinden und diese vertikal teilen: Der obere Centroid hat die grünen Instanzen, der untere die lila Instanzen. Die grünen Instanzen reichen bis in die Mitte der Daten.</p></li>
<li><p>Iteration 2: Die Centroids werden entsprechend der ihnen zugewiesenen Instanzen verschoben, sodass sie basierend auf der Zuweisung aus Iteration 1 mittig liegen. Hierdurch bewegt sich der gelbe Centroid nach oben rechts in die Ecke, wo er auch für die weiteren Iterationen stabil bleibt. Der blaue Centroid bewegt sich etwas nach unten, da der Großteil der zugewiesenen Instanzen am unteren Rand der Daten liegt. Der grüne Centroid bewegt sich etwas nach rechts, da ihm einige Instanzen aus der Mitte zugewiesen wurden. Der lila Centroid bewegt sich zwar kaum, bekommt aber mehr Instanzen am linken Rand zugewiesen, da sich der grüne Centroid nach rechts bewegt hat.</p></li>
<li><p>Iteration 3: Man sieht, wie das Ergebnis anfängt zu konvergieren. Der blaue Centroid liegt jetzt bereits zentral in der unteren mittleren Gruppe. Der grüne Centroid ist noch weiter nach rechts gewandert, sodass er bereits alle Instanzen in der mittleren Gruppe zugewiesen bekommt. Hierdurch hat der lila Centroid freies Spiel auf der linken Seite und bekommt die Instanzen in dieser Gruppe.</p></li>
<li><p>Iteration 4: Nach nur vier Iterationen haben wir bereits ein sehr gutes Ergebnis und eine klare Trennung in vier Cluster.</p></li>
</ul>
</div>
<div class="section" id="bestimmen-von-k">
<h3><span class="section-number">6.3.2. </span>Bestimmen von <span class="math notranslate nohighlight">\(k\)</span><a class="headerlink" href="#bestimmen-von-k" title="Permalink to this headline">¶</a></h3>
<p>Im obigen Beispiel haben wir <span class="math notranslate nohighlight">\(k=4\)</span> benutzt, also nach vier Clustern gesucht. Wie man einen geeigneten Wert für <span class="math notranslate nohighlight">\(k\)</span> bestimmt, haben wie noch offengelassen. Bevor wir uns im Detail damit befassen, betrachten wir erst einmal, wie sich das Ergebnis mit verschiedenen Werten für <span class="math notranslate nohighlight">\(k\)</span> ändert.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">f</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="n">k</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">6</span><span class="p">)</span>
<span class="k">for</span> <span class="nb">iter</span> <span class="ow">in</span> <span class="n">k</span><span class="p">:</span>
    <span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="nb">iter</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">kmeans</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">y_kmeans</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">centers</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">cluster_centers_</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="nb">min</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">iter</span><span class="o">-</span><span class="mi">3</span><span class="p">)),</span> <span class="p">(</span><span class="nb">iter</span><span class="p">)</span><span class="o">%</span><span class="k">2</span>]
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Ergebnis für $k=</span><span class="si">%i</span><span class="s1">$&#39;</span> <span class="o">%</span> <span class="nb">iter</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y_kmeans</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">centers</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">centers</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_06_5_0.png" src="../_images/kapitel_06_5_0.png" />
</div>
</div>
<p>Mit <span class="math notranslate nohighlight">\(k=2\)</span> bekommen wir ein sehr großes Cluster und ein kleineres Cluster oben rechts. Mit <span class="math notranslate nohighlight">\(k=3\)</span> zerfällt das große Cluster in zwei Gruppen, sodass wir ein Cluster oben rechts, ein Cluster in der Mitte und ein Cluster unten haben. Mit <span class="math notranslate nohighlight">\(k=4\)</span> zerfällt das mittlere Cluster in zwei Gruppen, so wie wir es im Beispiel auch hatten. Mit <span class="math notranslate nohighlight">\(k=5\)</span> zerfällt das Cluster in der Mitte in eine obere und eine untere Hälfte. Die Ergebnisse für <span class="math notranslate nohighlight">\(k=2, 3, 4\)</span> sehen alle sinnvoll aus. Man kann argumentieren, dass <span class="math notranslate nohighlight">\(k=2\)</span> ein gutes Ergebnis ist, da es eine große Lücke zwischen den Instanzen in der oberen Ecke und den anderen Daten gibt. Alle anderen Lücken sind kleiner. Man kann auch sagen, <span class="math notranslate nohighlight">\(k=3\)</span> sei gut, da es sich bei den Daten in der Mitte auch um ein langgezogenes Cluster handeln könnte. Bei <span class="math notranslate nohighlight">\(k=4\)</span> kann man argumentieren, dass alle Cluster eine ähnliche Anzahl von Daten und eine ähnliche Form haben, auch wenn einige Cluster recht nah beieinander liegen. Für <span class="math notranslate nohighlight">\(k=5\)</span> findet man jedoch keine Begründung mehr, warum dieses Ergebnis besser sein könnte als die anderen: Die Trennung des mittleren Clusters in zwei Cluster ergibt einfach keinen Sinn.</p>
<p>Wichtig ist auch die Erkenntnis, dass verschiedene Clusterergebnisse und insbesondere auch die Anzahl von Clustern zu sinnvollen Ergebnissen führen können und oft nicht klar ist, welches Ergebnis das beste ist. Welches Ergebnis man favorisiert, hängt dann vom Anwendungsfall ab: Durch eine manuelle Analyse der Bedeutung der Cluster kann man verstehen, welche Gruppen die Cluster repräsentieren, und damit die Ergebnisse einordnen, um zu verstehen, welches Ergebnis am besten passt.</p>
<p>Beim <span class="math notranslate nohighlight">\(k\)</span>-Means-Algorithmus gibt es verschiedene Aspekte, die die Auswahl von <span class="math notranslate nohighlight">\(k\)</span> beeinflussen:</p>
<ul class="simple">
<li><p>Das Domänenwissen über die Daten und den Anwendungsfall. Mithilfe des Domänenwissens kann man bewerten, ob die Daten innerhalb eines Clusters sich wirklich ähnlich sind oder auch ob sich verschiedene Cluster eventuell ähnlich sind. Wenn man bemerkt, dass die Daten innerhalb eines Clusters verschieden sind, ist das ein Indikator für die Erhöhung von <span class="math notranslate nohighlight">\(k\)</span>. Wenn man bemerkt, dass ähnliche Objekte in verschiedenen Clustern liegen, kann es helfen, <span class="math notranslate nohighlight">\(k\)</span> zu reduzieren. Je nach Anwendungsfall kann es auch möglich sein, dass eine bestimmte Anzahl von Clustern gesucht wird. Wenn man zum Beispiel eine binäre Trennung der Daten in zwei Gruppen erreichen möchte, ist durch den Anwendungsfall <span class="math notranslate nohighlight">\(k=2\)</span> bereits vorgegeben.</p></li>
<li><p>Visualisierungen sind ein mächtiges Werkzeug, um zu erkennen, wie gut die Cluster die Daten gruppieren, wie die Daten innerhalb eines Clusters verteilt sind und ob es klar erkennbare Lücken zwischen den Clustern gibt. Visualisierung ist zum Beispiel auch das Werkzeug, mit dem wir im obigen Beispiel verschiedene Werte von   beurteilt haben.</p></li>
</ul>
<p>Es gibt auch einen analytischen Ansatz, um <span class="math notranslate nohighlight">\(k\)</span> zu bestimmen, der auf der <em>Within-Sum-of-Squares</em> (WSS) basiert. Um zu verstehen, was WSS ist, müssen wir uns noch einmal an die Grundlagen des Algorithmus erinnern. Die Centroids werden in jeder Iteration so aktualisiert, dass sie das arithmetische Mittel der Daten in dem Cluster sind. Das bedeutet automatisch auch, dass die <em>Varianz</em> innerhalb eines Clusters vom Centroid minimiert wird. Die Varianz ist das Quadrat der Standardabweichung und kann berechnet werden, indem man die Summe der quadratischen Distanzen vom arithmeti-schen Mittel bildet. Da der Centroid auf das arithmetische Mittel gesetzt wird, minimiert die Aktualisierung der Position des Centroids also die <em>Summe der Quadrate</em> (Sum-of-Squares) <em>innerhalb</em> (Within) eines Clusters. Entsprechend ist WSS nichts anderes als ein Maß dafür, wie gut es gelingt, die Varianz innerhalb eines Clusters zu miniminieren und ist definiert als</p>
<div class="math notranslate nohighlight">
\[WSS = \sum_{i=1}^k\sum_{x \in X_i} d(x, C_i)^2.\]</div>
<p>Wir haben oben bereits anhand der Visualisierung diskutiert, wie gut verschiedene Werte von <span class="math notranslate nohighlight">\(k\)</span> sind. Da es für zufällig generierte Daten kein Domänenwissen gibt, können wir dies auch nicht nutzen, um die Cluster zu bewerten. Wir können verschiedene Werte von <span class="math notranslate nohighlight">\(k\)</span> jedoch mithilfe der WSS bewerten. Hierzu zeichnet man am besten ein einfaches Liniendiagramm, das die WSS für verschiedene Werte von <span class="math notranslate nohighlight">\(k\)</span> vergleicht.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># sklearn calls the WSS inertia</span>
<span class="n">inertia</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">k</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
<span class="k">for</span> <span class="nb">iter</span> <span class="ow">in</span> <span class="n">k</span><span class="p">:</span>
    <span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="nb">iter</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">kmeans</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">y_kmeans</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">centers</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">cluster_centers_</span>
    <span class="n">inertia</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">kmeans</span><span class="o">.</span><span class="n">inertia_</span> <span class="p">)</span>

<span class="n">fix</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Entwicklung von WSS&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">inertia</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;WSS&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;$k$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_06_7_0.png" src="../_images/kapitel_06_7_0.png" />
</div>
</div>
<p>Wie man sieht, gibt es einen starken Abfall der WSS von 2 nach 3 und von 3 nach 4. Ab 4 bleibt die WSS nahezu konstant. Außerdem sieht man, dass sich die Steigung bei den Werten 2, 3 und 4 stark ändert und die Kurve langsam abflacht. Diese Änderungen der Steigung nennt man auch <em>Ellenbogen</em> bzw. <em>Elbows</em>, da sie ein wenig dem Ellenbogen eines gebeugten Arms ähneln. Wenn wir die WSS nutzen, um <span class="math notranslate nohighlight">\(k\)</span> auszuwählen, finden wir geeignete Werte an diesen Ellenbogen. Der Grund liegt darin, dass wenn sich die Kurve nicht abflacht, sondern weiter gleich steil abfällt, die Verbesserung auch gleich bleibt. Es gibt also keinen guten Grund, ein <span class="math notranslate nohighlight">\(k\)</span> zu wählen, wo es keinen Ellenbogen gibt, da <span class="math notranslate nohighlight">\(k+1\)</span> genauso so viel besser ist als <span class="math notranslate nohighlight">\(k\)</span>, wie <span class="math notranslate nohighlight">\(k\)</span> als <span class="math notranslate nohighlight">\(k-1\)</span>.</p>
<p>Wie man sieht gibt es einen starken Abfall der WSS von 2 nach 3 und von 3 nach 4. Ab vier bleibt die WSS nahezu konstant. Außerdem sieht man das sicht die Steigung bei den Werten 2, 3, und 4 stark ändert und die Kurve langsam abflacht. Diese Änderung der Steigung nennt man auch <em>Ellenbogen</em>, bzw. <em>Elbows</em>, da sie ein wenig dem Ellenbogen eines gebeugten Arms ähneln. Wenn wir die WSS nutzen um <span class="math notranslate nohighlight">\(k\)</span> auszuwählen finden wir geeignete Werte an diesen Ellenbogen. Der Grund ist, dass wenn sich die Kurve nicht abflacht sondern weiter gleich steil abfällt, die Verbesserung auch gleichbleibt. Es gibt also keinen guten Grund, einen <span class="math notranslate nohighlight">\(k\)</span> zu wählen, wo es keinen Elbow gibt, da <span class="math notranslate nohighlight">\(k+1\)</span> genauso so viel besser ist als <span class="math notranslate nohighlight">\(k\)</span>, wie <span class="math notranslate nohighlight">\(k\)</span> als <span class="math notranslate nohighlight">\(k-1\)</span>.</p>
<p>Die WSS-Kurve fällt monoton ab. Das bedeutet, dass <span class="math notranslate nohighlight">\(WSS(k+1) \leq WSS(K)\)</span> für alle <span class="math notranslate nohighlight">\(k&gt;1\)</span>. Daher kann man <span class="math notranslate nohighlight">\(k\)</span> auch leider nicht als das Minimum der WSS auswählen, da dies einfach bei <span class="math notranslate nohighlight">\(k=|X|\)</span> erreicht wird. Der Grund dafür ist, dass die Varianz sich reduziert, wenn wir mehr Cluster hinzufügen. Das Minimum wird erreicht, wenn <span class="math notranslate nohighlight">\(d(x, C_{c(x)})=0\)</span> für alle <span class="math notranslate nohighlight">\(x \in X\)</span>. Unter der Annahme, dass es keine Instanzen mit identischen Werten gibt, braucht man also <span class="math notranslate nohighlight">\(|X|\)</span> Cluster, um das Minimum zu erreichen. Wie man in der obigen Grafik aber sieht, wird der Abfall der WSS schon deutlich früher sehr klein, im Beispiel bei <span class="math notranslate nohighlight">\(k=4\)</span>. Entsprechend kann man <span class="math notranslate nohighlight">\(k\)</span> gut durch eine visuelle Analyse der WSS auswählen, nicht jedoch durch einen Automatismus, der die WSS minimiert.</p>
<p>Die WSS-Kurve fällt monoton ab. Das bedeutet das <span class="math notranslate nohighlight">\(WSS(k+1) \leq WSS(k)\)</span> für alle <span class="math notranslate nohighlight">\(k&gt;1\)</span>. Daher kann man <span class="math notranslate nohighlight">\(k\)</span> auch leider nicht als das Minimum der WSS auswählen, da dies einfach bei <span class="math notranslate nohighlight">\(k=|X|\)</span> erreicht wird. Der Grund dafür ist, dass die Varianz sich reduziert, wenn wir mehr Cluster hinzufügen. Das Minimum wird erreicht, wenn <span class="math notranslate nohighlight">\(d(x, C_{c(x)})=0\)</span> für alle <span class="math notranslate nohighlight">\(x \in X\)</span>. Unter der Annahme das es keine Instanzen mit identischen Werten gibt, braucht man also <span class="math notranslate nohighlight">\(|X|\)</span> Cluster um das Minimum zu erreichen. Wie man in der obigen Grafik aber sieht, wird der Abfall der WSS schon deutlich früher sehr klein, im Beispiel bei <span class="math notranslate nohighlight">\(k=4\)</span>. Entsprechend kann man <span class="math notranslate nohighlight">\(k\)</span> gut durch eine visuelle Analyse der WSS auswählen, nicht jedoch durch einen Automatismus der die WSS minimiert.</p>
</div>
<div class="section" id="probleme-des-k-means-algorithmus">
<h3><span class="section-number">6.3.3. </span>Probleme des <span class="math notranslate nohighlight">\(k\)</span>-Means-Algorithmus<a class="headerlink" href="#probleme-des-k-means-algorithmus" title="Permalink to this headline">¶</a></h3>
<p>Obwohl das Konzept des <span class="math notranslate nohighlight">\(k\)</span>-Means-Algorithmus relativ einfach ist, bekommt man häufig gute Ergebnisse. Es gibt jedoch einige Probleme mit <span class="math notranslate nohighlight">\(k\)</span>-Means, die man kennen sollte.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(k\)</span>-Means ist sensitiv bezüglich der Auswahl der Startcentroids. Je nachdem wie die Centroids gewählt werden, kann es andere Clusterergebnisse geben. Man sagt dann, dass das Ergebnis nicht stabil ist. Entsprechend sollte man <span class="math notranslate nohighlight">\(k\)</span>-Means mehrfach mit verschiedenen Startcentroids ausführen. Wenn das Ergebnis instabil ist, ist das ein Indikator dafür, dass die Anzahl der Cluster eventuell anders gewählt werden sollte.</p></li>
<li><p>Ein schlechter Wert von <span class="math notranslate nohighlight">\(k\)</span> liefert möglicherweise schlechte Ergebnisse. Da man <span class="math notranslate nohighlight">\(k\)</span> manuell auswählen muss, sollte man die nötige Erfahrung mit dem Algorithmus haben, damit man <span class="math notranslate nohighlight">\(k\)</span> gut auswählen kann, um schlechte Ergebnisse zu vermeiden.</p></li>
<li><p>Alle Merkmale sollten einen ähnlichen Wertebereich haben, im Idealfall sogar denselben. Andernfalls können die Unterschiede im Wertebereich zu einer ungewollten Gewichtung der Merkmale führen. Betrachten wir hierfür ein Beispiel mit zwei Merkmalen: Alter in Jahren und das Bruttojahreseinkommen in Euro. Das Alter ist (grob) zwischen 0 und 100, das Einkommen in Euro ist selbst bei Minijobs schon bei <span class="math notranslate nohighlight">\(12\cdot 450=5.400\)</span> Euro, kann aber auch im sechsstelligen Bereich liegen. Das heißt, dass wenn man die Abstände zwischen zwei Instanzen berechnet, das Alter irrelevant ist. Alles, was zählt ,ist das Einkommen, da hier die Abstände deutlich größer sind. Der höhere Wertebereich sorgt also dafür, dass das Einkommen stärker gewichtet wird.</p></li>
<li><p>Weil die Cluster basierend auf der Distanz zugewiesen werden, tendieren die Cluster dazu, rund zu sein. Wenn die Cluster nicht rund sind, kann man sie häufig auch nicht gut durch Centroids beschreiben. Ein Beispiel dafür sind die zwei Halbmonde, die wir unten sehen. Man kann klar erkennen, dass es sich um zwei Halbkreise handelt, jeder sollte ein Cluster sein. <span class="math notranslate nohighlight">\(k\)</span>-Means kann diese Cluster aufgrund ihrer Form nicht finden.</p></li>
</ul>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_moons</span>
<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>

<span class="c1"># this generates our halfmoon data</span>
<span class="n">X</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">make_moons</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">kmeans</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">y_kmeans</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">centers</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">cluster_centers_</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Ergebnis des $k$-Means-Algorithmus für die Halbmonde&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y_kmeans</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">centers</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">centers</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_06_9_0.png" src="../_images/kapitel_06_9_0.png" />
</div>
</div>
</div>
</div>
<div class="section" id="em-clustering">
<h2><span class="section-number">6.4. </span>EM-Clustering<a class="headerlink" href="#em-clustering" title="Permalink to this headline">¶</a></h2>
<p>Wir könnten uns Städte auch als Zufallsvariablen vorstellen und die Häuser wären dann Instanzen dieser Zufallsvariablen. Die Zuweisung von Häusern zu Städten würde also von Wahrscheinlichkeitsverteilungen abhängen. Dies ist die Idee hinter einer Verallgemeinerung des <span class="math notranslate nohighlight">\(k\)</span>-Means-Algorithmus, die wir jetzt betrachten wollen. Unsere Rathäuser sind immer noch das Zentrum der Stadt. Aber wir berücksichtigen, dass Häuser, die sehr nah am Rathaus liegen, mit einer höheren Wahrscheinlichkeit zur Stadt gehören als solche, die weiter weg liegen. Wenn wir das weiterdenken, kann man dies zu einer Eigenschaft der Städte werden lassen: Es gibt Städte, die sich nicht sehr stark räumlich ausbreiten, und andere Städte, die eine große Fläche einnehmen. Wir wollen also die <em>Verteilung</em> der Häuser berücksichtigen. Daher nennt man diese Art zu Clustern auch <em>verteilungsbasiertes Clustern</em> (engl. <em>distribution-based clustering</em>).</p>
<p>Formal haben wir eine Anzahl von Clustern <span class="math notranslate nohighlight">\(k\)</span>, die durch Zufallsvariablen <span class="math notranslate nohighlight">\(C_1, ..., C_k\)</span> beschrieben werden. Wir können die Wahrscheinlichkeit, dass eine Instanz <span class="math notranslate nohighlight">\(x \in \mathcal{F}\)</span> zu einem Cluster <span class="math notranslate nohighlight">\(C_i\)</span> gehört, als <span class="math notranslate nohighlight">\(P(C_i = x), i=1, ..., k\)</span> berechnen. Jede Instanz hat also eine gewisse Wahrscheinlichkeit, zu jedem Cluster zu gehören. Da man aber trotzdem für jede Instanz wissen möchte, zu welchem Cluster sie jetzt gehört, kann man das <em>wahrscheinlichste</em> Cluster berechnen, also das Cluster <span class="math notranslate nohighlight">\(C_i\)</span>, das <span class="math notranslate nohighlight">\(P(C_i = x)\)</span> maximiert. Entsprechend ist die Zuweisung der Instanzen zu Clustern definiert als</p>
<div class="math notranslate nohighlight">
\[c(x) = \max_{i=1,..., k} P(C_i = x).\]</div>
<p>Auch wenn man das Konzept des EM-Algorithmus (EM - Expectation Maximization) mit beliebigen Verteilungen benutzen kann, benutzt man in der Regel normalverteilte Zufallsvariablen, um die Cluster zu beschreiben. Man spricht daher auch vom <em>Gaussian Mixture Model</em>. Diese Normalverteilungen sind <em>multivariat</em>, es wird also nicht nur eine einzelne Dimension beschrieben. Die Normalverteilung hat so viele Dimensionen, wie es Merkmale gibt. Eine univariate Normalverteilung kann man durch das arithmetische Mittel und die Standardab-weichung darstellen. Eine multivariate Normalverteilung wird durch einen Vektor von Mittelwerten und eine <em>Kovarianzmatrix</em> beschrieben. Die Kovarianz-matrix gibt die Beziehung zwischen den Varianzen der einzelnen Dimensionen an. Vereinfacht gesagt, kann man sich die Kovarianz als die Ausdehnung der Glockenform der Normalverteilung in die verschiedenen Richtungen vorstellen. Mathematisch betrachtet handelt es sich bei der Kovarianz um eine Ellipse: Die Form der Ellipse bestimmt die erwartete Abweichung der Daten in jede Richtung. Eine wichtige Eigenschaft der Kovarianzmatrix, die wir später noch brauchen, ist, dass es sich um quadratische Matrix handelt, bei der das obere rechte und das untere linke Dreieck symmetrisch sind. Es gibt also <span class="math notranslate nohighlight">\(\frac{d\cdot(d+1)}{2}\)</span> freie Parameter in der Kovarianzmatrix. Wenn wir also <span class="math notranslate nohighlight">\(d\)</span> Merkmale haben, gibt es <span class="math notranslate nohighlight">\(d+\frac{d\cdot(d+1)}{2}\)</span> Parameter, die wir benötigen, um eine entsprechende multivariate Normalverteilung zu beschreiben: <span class="math notranslate nohighlight">\(d\)</span> Mittelwerte und <span class="math notranslate nohighlight">\(\frac{d\cdot(d+1)}{2}\)</span> Parameter für die <span class="math notranslate nohighlight">\(d \times d\)</span>-Kovarianzmatrix. Insgesamt haben wir also <span class="math notranslate nohighlight">\(k \cdot (d+\frac{d\cdot(d+1)}{2})\)</span> Parameter, wenn wir <span class="math notranslate nohighlight">\(k\)</span> Cluster beschreiben wollen.</p>
<p>Wir betrachten jetzt die gleichen Daten wie eben, nur dass wir uns das Ergebnis des EM-Clustering mit <span class="math notranslate nohighlight">\(k=4\)</span> Normalverteilungen anschauen. Die Mittelwerte sind als große graue Punkte markiert, die Kovarianzen als Ellipsen in der Farbe der jeweiligen Cluster.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib</span> <span class="k">as</span> <span class="nn">mpl</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">from</span> <span class="nn">sklearn.mixture</span> <span class="kn">import</span> <span class="n">GaussianMixture</span>

<span class="c1"># we silence some annyoing warnings</span>
<span class="c1"># if something is not working properly, remove this part and restart the kernel</span>
<span class="k">def</span> <span class="nf">warn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">pass</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">warn</span> <span class="o">=</span> <span class="n">warn</span>

<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;indigo&#39;</span><span class="p">,</span> <span class="s1">&#39;navy&#39;</span><span class="p">,</span> <span class="s1">&#39;mediumseagreen&#39;</span><span class="p">,</span> <span class="s1">&#39;gold&#39;</span><span class="p">]</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y_true</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">center_box</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span>
                       <span class="n">cluster_std</span><span class="o">=</span><span class="mf">0.04</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>


<span class="n">em</span> <span class="o">=</span> <span class="n">GaussianMixture</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">em</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">y_em</span> <span class="o">=</span> <span class="n">em</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">centers</span> <span class="o">=</span> <span class="n">em</span><span class="o">.</span><span class="n">means_</span>

<span class="n">f</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Daten (ohne Clustering)&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">);</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Ergebnis des EM-Clustering&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y_em</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">centers</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">centers</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">);</span>
<span class="c1"># this code is for plotting the elipses</span>
<span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">color</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">colors</span><span class="p">):</span>
    <span class="n">v</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigh</span><span class="p">(</span><span class="n">em</span><span class="o">.</span><span class="n">covariances_</span><span class="p">[</span><span class="n">n</span><span class="p">][:</span><span class="mi">2</span><span class="p">,:</span><span class="mi">2</span><span class="p">])</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">angle</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arctan2</span><span class="p">(</span><span class="n">u</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">u</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">angle</span> <span class="o">=</span> <span class="mi">180</span> <span class="o">*</span> <span class="n">angle</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span>  <span class="c1"># convert to degrees</span>
    <span class="n">v</span> <span class="o">=</span> <span class="mf">2.</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
    <span class="n">ell</span> <span class="o">=</span> <span class="n">mpl</span><span class="o">.</span><span class="n">patches</span><span class="o">.</span><span class="n">Ellipse</span><span class="p">(</span><span class="n">em</span><span class="o">.</span><span class="n">means_</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="p">:</span><span class="mi">2</span><span class="p">],</span> <span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                              <span class="mi">180</span> <span class="o">+</span> <span class="n">angle</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">)</span>
    <span class="n">ell</span><span class="o">.</span><span class="n">set_clip_box</span><span class="p">(</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">bbox</span><span class="p">)</span>
    <span class="n">ell</span><span class="o">.</span><span class="n">set_alpha</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">add_artist</span><span class="p">(</span><span class="n">ell</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_06_11_0.png" src="../_images/kapitel_06_11_0.png" />
</div>
</div>
<p>Auch wenn die Beschreibung der Cluster durch Zufallsvariablen komplexer ist als durch Centroids, sieht man in der obigen Grafik den Vorteil: Die Kovarianzen erlauben es, die Verteilung der Daten in die Clusterbeschreibung mit aufzunehmen. Wir haben also mehr Informationen über die Daten als Teil der Clusterbeschreibung.</p>
<div class="section" id="id1">
<h3><span class="section-number">6.4.1. </span>Der Algorithmus<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<p>Man nennt diese Art zu clustern EM-Clustering, weil sie auf dem <em>Expectation-Maximization-(EM-)Algorithmus</em> basiert. Der Algorithmus ist im Allgemeinen ähnlich zu <span class="math notranslate nohighlight">\(k\)</span>-Means: Wir starten mit einer zufälligen Initialisierung der Zufallsvariablen und verbessern die Beschreibung der Daten durch die Zufallsvariablen iterativ. Beim EM-Algorithmus müssen also für jedes Cluster die Mittelwerte und die Kovarianzmatrizen zufällig initialisiert und stückweise verbessert werden. Die Verbesserung basiert auf der Wahrscheinlichkeit (engl. <em>likelihood</em>), dass die Daten Instanzen dieser Zufallsvariablen sein könnten. Im Folgenden betrachten wir nicht die vollständige mathematische Beschreibung des EM-Algorithmus, sondern nur eine vereinfachte Version, in der wir die Mittelwerte aktualisieren und die Kovarianzen ignorieren.</p>
<ol class="simple">
<li><p>Wähle zufällig <span class="math notranslate nohighlight">\(k\)</span> Normalverteilungen <span class="math notranslate nohighlight">\(C_1 \sim (\mu_1, \Sigma_1), ..., C_k \sim (\mu_k, \Sigma_k)\)</span>, wobei <span class="math notranslate nohighlight">\(\mu_i \in \mathcal{F}\)</span> die Mittelwerte und <span class="math notranslate nohighlight">\(\Sigma_i \in \mathcal{F}\times\mathcal{F}\)</span> die Kovarianzmatrizen sind.</p></li>
<li><p>Expectation-Schritt: Bestimme die Gewichte <span class="math notranslate nohighlight">\(w_i(x) = \frac{p(x|\mu_i, \Sigma_i)}{\sum_{j=1}^k p(x|\mu_j, \Sigma_j)}\)</span> für alle Instanzen <span class="math notranslate nohighlight">\(x \in X\)</span> und Cluster <span class="math notranslate nohighlight">\(i=1, ..., k\)</span>.</p></li>
<li><p>Maximization-Schritt: Aktualisiere die Mittelwerte, sodass <span class="math notranslate nohighlight">\(\mu_i = \frac{1}{|X|}\sum_{x \in X} w_i(x)\cdot x\)</span> für jedes Cluster <span class="math notranslate nohighlight">\(i=1, ..., k\)</span>.
4, Wiederhole die Schritte 2 und 3, bis</p>
<ul class="simple">
<li><p>das Ergebnis konvergiert, also sich die Cluster <span class="math notranslate nohighlight">\(C_1, ..., C_k\)</span> nicht mehr verändern oder</p></li>
<li><p>eine vorher festgelegte Höchstanzahl an Iterationen erreicht ist.</p></li>
</ul>
</li>
</ol>
<p>Der größte Unterschied zwischen dem EM-Algorithmus und dem <span class="math notranslate nohighlight">\(k\)</span>-Means-Algorithmus sind die Gewichte. Diese Gewichte geben die Wahrscheinlichkeit an, dass eine Instanz <span class="math notranslate nohighlight">\(x \in X\)</span> zu einem bestimmten Cluster gehört. Der Wert <span class="math notranslate nohighlight">\(w_i(x)=0,9\)</span> bedeutet also, dass die Instanz <span class="math notranslate nohighlight">\(x\)</span> mit 90% Wahrscheinlichkeit zu Cluster <span class="math notranslate nohighlight">\(i\)</span> gehört. Die Zuweisung der Instanzen zu den Clustern erfolgt nach der oben beschriebenen Regel: Jede Instanz wird dem Cluster zugewiesen, das die Wahrscheinlichkeit maximiert, also</p>
<div class="math notranslate nohighlight">
\[c(x) = argmax_{i=1, ..., k} w_i(x).\]</div>
<p>Durch die Berechnung der Wahrscheinlichkeit für jedes Cluster kann man beim EM-Clustering die Unsicherheit der Ergebnisse gut nachvollziehen. Wenn eine Instanz für verschiedene Cluster eine hohe Wahrscheinlichkeit hat, erkennt man, dass diese Zuweisung unsicher ist. Diese Art zu clustern nennt man auch <em>Soft Clustering</em>, da die Instanzen nicht “hart” genau einem Cluster zugewiesen werden, sondern stattdessen eine “weiche” Zuweisung über Wahrscheinlichkeiten erfolgt.</p>
<p>Um ein tieferes Verständnis für den EM-Algorithmus zu bekommen, betrachten wir jetzt an einem Beispiel, wie die Cluster aktualisiert werden.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">f</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="k">for</span> <span class="nb">iter</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">cur_iter</span> <span class="o">=</span> <span class="nb">iter</span><span class="o">*</span><span class="mi">20</span><span class="o">+</span><span class="mi">1</span>
    <span class="n">em</span> <span class="o">=</span> <span class="n">GaussianMixture</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="n">cur_iter</span><span class="p">,</span> <span class="n">init_params</span><span class="o">=</span><span class="s1">&#39;random&#39;</span><span class="p">)</span>
    <span class="n">em</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">y_em</span> <span class="o">=</span> <span class="n">em</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">centers</span> <span class="o">=</span> <span class="n">em</span><span class="o">.</span><span class="n">means_</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="nb">min</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">iter</span><span class="o">-</span><span class="mi">1</span><span class="p">)),</span> <span class="p">(</span><span class="nb">iter</span><span class="p">)</span><span class="o">%</span><span class="k">2</span>]
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Iteration </span><span class="si">%i</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">cur_iter</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y_em</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">centers</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">centers</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">);</span>
    <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">color</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">colors</span><span class="p">):</span>
        <span class="n">v</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigh</span><span class="p">(</span><span class="n">em</span><span class="o">.</span><span class="n">covariances_</span><span class="p">[</span><span class="n">n</span><span class="p">][:</span><span class="mi">2</span><span class="p">,:</span><span class="mi">2</span><span class="p">])</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">angle</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arctan2</span><span class="p">(</span><span class="n">u</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">u</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">angle</span> <span class="o">=</span> <span class="mi">180</span> <span class="o">*</span> <span class="n">angle</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span>  <span class="c1"># convert to degrees</span>
        <span class="n">v</span> <span class="o">=</span> <span class="mf">2.</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
        <span class="n">ell</span> <span class="o">=</span> <span class="n">mpl</span><span class="o">.</span><span class="n">patches</span><span class="o">.</span><span class="n">Ellipse</span><span class="p">(</span><span class="n">em</span><span class="o">.</span><span class="n">means_</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="p">:</span><span class="mi">2</span><span class="p">],</span> <span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                                  <span class="mi">180</span> <span class="o">+</span> <span class="n">angle</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">)</span>
        <span class="n">ell</span><span class="o">.</span><span class="n">set_clip_box</span><span class="p">(</span><span class="n">ax</span><span class="o">.</span><span class="n">bbox</span><span class="p">)</span>
        <span class="n">ell</span><span class="o">.</span><span class="n">set_alpha</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">add_artist</span><span class="p">(</span><span class="n">ell</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_06_13_0.png" src="../_images/kapitel_06_13_0.png" />
</div>
</div>
<p>Im Detail passiert Folgendes:</p>
<ul class="simple">
<li><p>Iteration 1: Wir starten mit vier Normalverteilungen, die nahezu identisch sind. Der Mittelwert liegt in der Mitte der Daten und die Kovarianzen sind in der Diagonale verlängerte Ellipsen, die sich geringfügig voneinander unterscheiden. Man sieht zum Beispiel, dass die grüne Ellipse etwas weiter nach links unten geht und die gelbe etwas weiter nach rechts oben. Dies spiegelt sich auch schon in der Zuweisung der Instanzen zu den Clustern wider: Links unten ist das grüne Cluster, rechts oben das gelbe, die anderen Farben sind kaum zu erkennen aufgrund der Überschneidung.</p></li>
<li><p>Iteration 21: Nach 21 Iterationen sieht man schon, wie sich die Normalverteilungen voneinander trennen. Die gelbe Verteilung wird nach rechts oben gezogen, hat aber immer noch eine sehr große Kovarianzellipse. Die grüne wurde zwischen die Instanzen links und unten gezogen. Die blaue liegt in der Mitte der Daten, hat jedoch jetzt eine deutlich kleinere Kovarianz. Die lila Verteilung hat sich kaum bewegt und die Kovarianz ist immer noch sehr hoch.</p></li>
<li><p>Iteration 41: Man sieht, wie die Verteilungen langsam gegen ihre Ziele konvergieren. Die gelbe Verteilung liegt jetzt fest rechts oben in der Ecke und hat sich dort an die Instanzen angepasst. Die Kovarianz der blauen Verteilung ist auch weiter geschrumpft, sodass diese jetzt nur noch die Instanzen in der Mitte überdeckt. Hierdurch wurde die bisher noch “unentschlossene” lila Verteilung nach links gedrückt, wo sie dank einer relativ kleinen Kovarianz anfängt, die grüne Verteilung zu dominieren. Als Konsequenz beginnt die grüne Verteilung, sich zu den Instanzen am unteren Rand zu bewegen.</p></li>
<li><p>Iteration 61: Die Cluster sind konvergiert: Die lila Verteilung ist jetzt fest bei den Instanzen links und die grüne ist vollständig nach unten gewandert.</p></li>
</ul>
<p>Im Beispiel erkennt man zwei wichtige Eigenschaften des EM-Clustering. Zum einen konvergiert der Algorithmus deutlich langsamer als <span class="math notranslate nohighlight">\(k\)</span>-Means. Dies liegt daran, dass mehr Parameter optimiert werden müssen, also nicht nur die Mittelwerte, sondern auch die Kovarianzen. Außerdem bremst das Soft Clustering die Konvergenz: Hierdurch bewegen sich die Mittelwerte nur langsam, da sie auch von anderen Instanzen, die eigentlich schon anderen Clustern zugeordnet werden, noch angezogen werden, wenn auch schwächer. Zum anderen sieht man, dass die Cluster möglicherweise keine zusammenhängende Region beschreiben. In Iteration 41 gibt es einige grüne Instanzen am linken Rand, der Rest befindet sich unten. Dazwischen liegen Instanzen, die dem lila Cluster zugeordnet sind. Es gibt also zwei getrennte Regionen im grünen Cluster. Diese Trennung wird durch die Form der Kovarianzmatrizen ermöglicht. Da die Kovarianz der grünen Verteilung größer ist, haben Instanzen, die weit weg vom Mittel liegen, noch eine relativ hohe Wahrscheinlichkeit, zur grünen Verteilung zu gehören. Die Kovarianz der lila Verteilung ist deutlich kleiner, sodass die Wahrscheinlichkeit mit der Distanz stärker abnimmt. Derart getrennte Regionen sind ungewöhnlich und mit den meisten Clusteralgorithmen gar nicht möglich. Beim  <span class="math notranslate nohighlight">\(k\)</span>-Means-Algorithmus ist dies zum Beispiel ausgeschlossen, da sich die Distanz für jeden Centroid in alle Richtungen gleich verändert.</p>
<blockquote>
<div><p><strong>Bemerkung:</strong></p>
<p>Die Gemeinsamkeiten des <span class="math notranslate nohighlight">\(k\)</span>-Means-Algorithmus und des EM-Clustering gehen noch über die obige Beschreibung hinaus. Genau genommen ist <span class="math notranslate nohighlight">\(k\)</span>-Means nur ein Spezialfall des EM-Clustering in dem Voronoi-Zellen <a class="footnote-reference brackets" href="#voronoi" id="id2">1</a> zur Modellierung der Wahrscheinlichkeiten genutzt werden.</p>
</div></blockquote>
</div>
<div class="section" id="id3">
<h3><span class="section-number">6.4.2. </span>Bestimmen von <span class="math notranslate nohighlight">\(k\)</span><a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<p>Genau wie beim <span class="math notranslate nohighlight">\(k\)</span>-Means-Clustern muss auch beim EM-Clustern die Anzahl der Cluster durch den Benutzer vorgegeben werden. Der Ansatz ist ähnlich: Domänenwissen und Visualisierungen sind auch hier gute Mittel, um <span class="math notranslate nohighlight">\(k\)</span> zu bestimmen.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">f</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="n">k</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="k">for</span> <span class="nb">iter</span> <span class="ow">in</span> <span class="n">k</span><span class="p">:</span>
    <span class="n">em</span> <span class="o">=</span> <span class="n">GaussianMixture</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="nb">iter</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">em</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">y_em</span> <span class="o">=</span> <span class="n">em</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">centers</span> <span class="o">=</span> <span class="n">em</span><span class="o">.</span><span class="n">means_</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="nb">min</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">iter</span><span class="o">-</span><span class="mi">2</span><span class="p">)),</span> <span class="p">(</span><span class="nb">iter</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">%</span><span class="k">2</span>]
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Ergebnis für $k=</span><span class="si">%i</span><span class="s1">$&#39;</span> <span class="o">%</span> <span class="nb">iter</span><span class="p">)</span>
    <span class="c1"># we use a bit of an indirect way to define the colors, because they</span>
    <span class="c1"># otherwise sometimes did not match with the ellipses</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="p">[</span><span class="n">colors</span><span class="p">[</span><span class="n">y</span><span class="p">]</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">y_em</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">centers</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">centers</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">);</span>
    <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">color</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">colors</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="nb">iter</span><span class="p">]):</span>
        <span class="n">v</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigh</span><span class="p">(</span><span class="n">em</span><span class="o">.</span><span class="n">covariances_</span><span class="p">[</span><span class="n">n</span><span class="p">][:</span><span class="mi">2</span><span class="p">,:</span><span class="mi">2</span><span class="p">])</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">angle</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arctan2</span><span class="p">(</span><span class="n">u</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">u</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">angle</span> <span class="o">=</span> <span class="mi">180</span> <span class="o">*</span> <span class="n">angle</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span>  <span class="c1"># convert to degrees</span>
        <span class="n">v</span> <span class="o">=</span> <span class="mf">2.</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
        <span class="n">ell</span> <span class="o">=</span> <span class="n">mpl</span><span class="o">.</span><span class="n">patches</span><span class="o">.</span><span class="n">Ellipse</span><span class="p">(</span><span class="n">em</span><span class="o">.</span><span class="n">means_</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="p">:</span><span class="mi">2</span><span class="p">],</span> <span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                                  <span class="mi">180</span> <span class="o">+</span> <span class="n">angle</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">)</span>
        <span class="n">ell</span><span class="o">.</span><span class="n">set_clip_box</span><span class="p">(</span><span class="n">ax</span><span class="o">.</span><span class="n">bbox</span><span class="p">)</span>
        <span class="n">ell</span><span class="o">.</span><span class="n">set_alpha</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">add_artist</span><span class="p">(</span><span class="n">ell</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_06_15_0.png" src="../_images/kapitel_06_15_0.png" />
</div>
</div>
<p>Man kann gut erkennen, dass eine einzelne Normalverteilung nicht ausreicht, um die Daten zu beschreiben. Die Ergebnisse für <span class="math notranslate nohighlight">\(k=2, 3, 4\)</span> sind besser. Man sieht jedoch, dass nur für <span class="math notranslate nohighlight">\(k=4\)</span>  die Verteilung der Instanzen innerhalb der Cluster wirklich der Form der Kovarianzellipsen entspricht, was dafür spricht, dass dies das beste Ergebnis ist.</p>
<p>Es gibt auch einen analytischen Ansatz für die Bestimmung von <span class="math notranslate nohighlight">\(k\)</span> basierend auf dem <em>Bayesian Information Criterion</em> (BIC). Ähnlich wie bei WSS ist BIC auch ein Maß dafür, wie gut der Algorithmus sein Optimierungsziel erreicht. Der EM-Algorithmus probiert, die Wahrscheinlichkeit des Ergebnisses zu maximieren. Formal macht man das mit der <em>Likelihood-Funktion</em> <span class="math notranslate nohighlight">\(\hat{L}(C_1, ..., C_k; X)\)</span>, die wir bei der Beschreibung des EM-Algorithmus ausgespart haben und auch hier nicht im Detail definieren wollen. Der Wert der Likelihood-Funktion wird höher, wenn die Cluster eine bessere Erklärung für die Daten sind. Ähnlich zum monotonen Abfall der WSS ist auch die Likelihood-Funktion in der Regel monoton: Mit mehr Clustern bekommt man einen höheren Wert. BIC wird jedoch nicht nur über die Likelihood-Funktion berechnet. Zusätzlich wird die Komplexität des berechneten Modells für die Cluster berücksichtigt. Zu Beginn des Kapitels haben wir bereits gezeigt, dass man <span class="math notranslate nohighlight">\(k' = k \cdot (d+\frac{d\cdot(d+1)}{2})\)</span> Parameter braucht, um <span class="math notranslate nohighlight">\(k\)</span> Cluster für <span class="math notranslate nohighlight">\(d\)</span> Merkmale durch multivariate Normalverteilungen zu beschreiben. Ockhams Rasiermesser besagt, dass die einfachste Erklärung meistens die beste ist <a class="footnote-reference brackets" href="#occam" id="id4">2</a>. Das bedeutet hier, dass wenn man einen ähnlichen Wert der Likelihood-Funktion mit weniger Parametern erreichen kann, man dieses kleinere Modell verwenden sollte. Komplexere Beschreibungen, also mehr Parameter, die aber die Likelihood-Funktion nur noch wenig verbessern, könnten hingegen Overfitting sein. Auf Grundlage dieser Konzepte ist BIC definiert als</p>
<div class="math notranslate nohighlight">
\[BIC = \log(|X|)\cdot k' - 2\cdot \log(\hat{L}(C_1, ..., C_k; X)).\]</div>
<p>BIC wird also größer mit mehr Parametern und mehr Trainingsdaten und fällt ab, wenn sich die Likelihood-Funktion verbessert. Daher sollte man probieren, den BIC zu minimieren. Im Vergleich zu WSS ist BIC nicht monoton, sondern es gibt ein Minimum. Dies ist möglich, da BIC die Modellkomplexität bestraft. Ab einem gewissen Punkt ist die Strafe für ein komplexeres Modell größer als die Verbesse-rung der Likelihood-Funktion und das Minimum ist erreicht. Hieraus folgt, dass es basierend auf BIC sogar eine <em>optimale</em> Anzahl von Clustern gibt, die man automatisch als den Wert von <span class="math notranslate nohighlight">\(k\)</span>, bei dem BIC minimiert wird, bestimmen kann. Auch das können wir uns am Beispiel anschauen.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ks</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">11</span><span class="p">)</span>
<span class="n">bic</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">ks</span><span class="p">:</span>
    <span class="n">em</span> <span class="o">=</span> <span class="n">GaussianMixture</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">k</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">em</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">y_em</span> <span class="o">=</span> <span class="n">em</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">centers</span> <span class="o">=</span> <span class="n">em</span><span class="o">.</span><span class="n">means_</span>
    <span class="n">bic</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">em</span><span class="o">.</span><span class="n">bic</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Entwicklung des BIC&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ks</span><span class="p">,</span> <span class="n">bic</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;BIC&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;$k$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">ks</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_06_17_0.png" src="../_images/kapitel_06_17_0.png" />
</div>
</div>
<p>Das Minimum des BIC wird für <span class="math notranslate nohighlight">\(k=4\)</span> erreicht. Dies bedeutet, dass vier Cluster der optimale Trade-off zwischen der Anzahl der Modellparameter und der Qualität der Cluster sind. Bitte beachten Sie, dass man die absoluten Werte des BIC ignorieren sollte (im Bild also die Werte der y-Achse). Ob BIC positiv oder negativ ist oder die Werte in der Größenordnung <span class="math notranslate nohighlight">\(10^2\)</span> oder <span class="math notranslate nohighlight">\(10^5\)</span> liegen, hängt eher von den Daten ab als von der Qualität der Ergebnisse. Entsprechend kann man sogar die Skala der y-Achse ausblenden, ohne wertvolle Informationen zu verlieren.</p>
<blockquote>
<div><p><strong>Bermerkung:</strong></p>
<p>BIC ist aus dem <em>Akitake Information Criterion</em> (AIC) abgeleitet, das als</p>
<div class="math notranslate nohighlight">
\[AIC = 2\cdot k' - 2\cdot \log(\hat{L}(C_1, ..., C_k; X))\]</div>
<p>definiert ist. Das AIC ist eine Folgerung aus der Kullback-Leibler-Divergenz, einem informationstheoretischem Maß für den Unterschied zwischen zwei Zufallsvariablen. Dies ist auch der Grund, warum der Logarithmus in der AIC-Definition und auch in der BIC-Definition verwendet wird. Der einzige Unterschied zwischen AIC und BIC ist der erste Term: Während das AIC einen festen Faktor von 2 hat, wird beim BIC die Modellkomplexität noch mit dem Logarithmus der Anzahl der Trainingsdaten gewichtet.</p>
</div></blockquote>
</div>
<div class="section" id="probleme-des-em-clustering">
<h3><span class="section-number">6.4.3. </span>Probleme des EM-Clustering<a class="headerlink" href="#probleme-des-em-clustering" title="Permalink to this headline">¶</a></h3>
<p>EM-Clustering löst zwei Probleme des <span class="math notranslate nohighlight">\(k\)</span>-Means-Algorithmus: Unterschiedliche Wertebereiche der Merkmale können durch die Kovarianzmatrizen berücksichtigt werden. Hierdurch lassen sich auch ellipsoide Cluster beschreiben, was mächtiger als die eher runde Form beim <span class="math notranslate nohighlight">\(k\)</span>-Means-Clustering ist. Die anderen Probleme des <span class="math notranslate nohighlight">\(k\)</span>-Means-Algorithmus hat man auch weiterhin mit dem EM-Clustering.</p>
<p>EM Clustering löst zwei Probleme des <span class="math notranslate nohighlight">\(k\)</span>-Means Algorithmus: Unterschiedliche Wertebereiche der Merkmale können durch die Kovarianzmatrizen berücksichtigt werden. Hierdurch lassen sich auch ellipsoide Cluster beschreiben, was mächtiger als die eher runde Form beim <span class="math notranslate nohighlight">\(k\)</span>-Means Clustering ist. Die anderen Probleme des <span class="math notranslate nohighlight">\(k\)</span>-Means Algorithmus hat man auch weiterhin mit dem EM Clustering.</p>
<ul class="simple">
<li><p>EM-Clustering ist sensitiv bezüglich der Startwerte. Eine schlechte Startposition kann dazu führen, dass der Algorithmus nur sehr langsam konvergiert oder sogar ein schlechtes Ergebnis liefert. Ein guter Ansatz, das EM-Clustering zu initialisieren, ist es, die Centroids vom <span class="math notranslate nohighlight">\(k\)</span>-Means-Clustering als Startwerte für die Mittelwerte der Cluster zu nutzen.</p></li>
<li><p>Die Wahl von <span class="math notranslate nohighlight">\(k\)</span> kann zu schlechten Ergebnissen führen. Auch wenn das BIC ein guter Ansatz für die Auswahl von   ist, heißt es nicht zwingend, dass man ein gutes Ergebnis findet. Es könnte zum Beispiel möglich sein, dass man sehr viele Cluster bräuchte, um das Optimum des BIC zu erreichen. In diesem Fall sollte man BIC ähnlich wie WSS benutzen und stattdessen nach Ellenbogen bei einer kleineren Anzahl von Clustern suchen.</p></li>
<li><p>EM-Clustering ist nur für Cluster, die sich durch Normalverteilungen beschreiben lassen, gut geeignet, was eine etwa ellipsoide Form der Cluster voraussetzt. Die Halbmonde könnten auch vom EM-Algorithmus nicht richtig gruppiert werden.</p></li>
</ul>
</div>
</div>
<div class="section" id="dbscan">
<h2><span class="section-number">6.5. </span>DBSCAN<a class="headerlink" href="#dbscan" title="Permalink to this headline">¶</a></h2>
<p>Bisher haben wir immer die Rathäuser als Referenzpunkt für die Definition von Städten benutzt. Wenn wir aus dem Fenster schauen, erkennen wir jedoch, dass die Nachbarhäuser zur gleichen Stadt gehören, auch ohne zu wissen, wo das Rathaus liegt. Das Gleiche gilt für die Nachbarn Ihrer Nachbarn usw. Im Endeffekt sind alle Ihre direkten oder indirekten Nachbarn Teil der gleichen Stadt. Dies ist die Grundidee des <em>dichtebasierten Clustering</em> (engl. <em>density-based clustering</em>).</p>
<p>Dieser Ansatz ist grundverschieden vom <span class="math notranslate nohighlight">\(k\)</span>-Means-Algorithmus und vom EM-Clustering, weil es keine Formel gibt, mit der man angeben kann, welche Instanzen zu einem Cluster gehören. Stattdessen beschreiben die Instanzen die Cluster: Wenn wir wissen wollen, welche Instanzen zum gleichen Cluster gehören, müssen wir uns die Nachbarschaften anschauen.</p>
<p>Der DBSCAN-Algorithmus (Density-Based Spatial Clustering of Applications with Noise) ist ein weitverbreiteter dichtebasierter Clusteralgorithmus. Auf unseren Beispieldaten liefert DBSCAN das folgende Ergebnis.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">DBSCAN</span>

<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;indigo&#39;</span><span class="p">,</span> <span class="s1">&#39;navy&#39;</span><span class="p">,</span> <span class="s1">&#39;mediumseagreen&#39;</span><span class="p">,</span> <span class="s1">&#39;gold&#39;</span><span class="p">]</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y_true</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">center_box</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span>
                       <span class="n">cluster_std</span><span class="o">=</span><span class="mf">0.04</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>


<span class="n">dbscan</span> <span class="o">=</span> <span class="n">DBSCAN</span><span class="p">(</span><span class="n">eps</span><span class="o">=</span><span class="mf">0.03</span><span class="p">,</span> <span class="n">min_samples</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">dbscan</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="c1"># noise is marked with a negative value for the cluster number</span>
<span class="n">noise_mask</span> <span class="o">=</span> <span class="n">dbscan</span><span class="o">.</span><span class="n">labels_</span><span class="o">&lt;</span><span class="mi">0</span>

<span class="n">f</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Daten für Clustering&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;steelblue&#39;</span><span class="p">);</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Ergebnis von DBSCAN-Clustering&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">dbscan</span><span class="o">.</span><span class="n">labels_</span><span class="p">[</span><span class="o">~</span><span class="n">noise_mask</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;Grey&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_06_19_0.png" src="../_images/kapitel_06_19_0.png" />
</div>
</div>
<p>Wir können dieselben vier Cluster wie auch bisher erkennen. Es gibt jedoch auch einige Instanzen die mit einem grauen Kreuz markiert sind. Diese Instanzen sind keinem Cluster zugeordnet und werden als <em>Rauschen</em> (engl. <em>noise</em>) identifiziert. Dies kann passieren, wenn es Instanzen gibt, die sich nicht in einer dichten Nachbarschaft befinden. In unserer Analogie sind das die Häuser, die sich außerhalb der Stadtgrenzen befinden, zum Beispiel ein Bauernhof an einer Landstraße.</p>
<div class="section" id="id5">
<h3><span class="section-number">6.5.1. </span>Der Algorithmus<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h3>
<p>Der DBSCAN-Algorithmus basiert auf dem Konzept von <em>dichten Nachbarschaften</em>. Eine Nachbarschaft wird über die Distanz <span class="math notranslate nohighlight">\(\epsilon \in \mathbb{R}\)</span> definiert, sodass die Nachbarn einer Instanz <span class="math notranslate nohighlight">\(x \in X\)</span> definiert sind als <span class="math notranslate nohighlight">\(neighbors(x) = \{x' \in X: d(x, x') \leq \epsilon\}\)</span>. Eine Nachbarschaft heißt <em>dicht</em>, wenn sie mehr als <span class="math notranslate nohighlight">\(minPts \in \mathbb{N}\)</span> Instanzen beinhaltet. Alle Instanzen, die eine dichte Nachbarschaft haben, nennt man <em>Kernpunkte</em> (engl. <em>core points</em>). Wir nutzen die Notation <span class="math notranslate nohighlight">\(core(C) = \{x \in X: |neighbors(x)| \geq minPts\}\)</span>, um alle Kernpunkte innerhalb eines Clusters <span class="math notranslate nohighlight">\(C \subseteq X\)</span> zu beschreiben, und <span class="math notranslate nohighlight">\(core(X)\)</span> für alle Kernpunkte.</p>
<p>Sobald man alle Kernpunkte bestimmt hat, kann man ein Cluster <em>wachsen</em> lassen. Hierzu wählt man einen beliebigen Kernpunkt aus und definiert, dass dieser Kernpunkt zum ersten Cluster gehört. Dann fügt man alle Nachbarn dem Cluster hinzu. Für alle Nachbarn, die ebenfalls Kernpunkte sind, wird dies wiederholt, bis man irgendwann alle Nachbarn von allen Kernpunkten im Cluster hat. Dann wählt man den nächsten Kernpunkt, der noch zu keinem Cluster gehört aus, und lässt das nächste Cluster wachsen.</p>
<ol class="simple">
<li><p>Setze <span class="math notranslate nohighlight">\(i=1\)</span> als aktuelle Nummer des Clusters.</p></li>
<li><p>Wähle einen Kernpunkt aus, der noch keinem Cluster zugeordnet ist, um ein neues Cluster zu initialisieren, also <span class="math notranslate nohighlight">\(x \in core(X) \setminus \bigcup_{j_i} C_j\)</span> und setze <span class="math notranslate nohighlight">\(C_i = \{x\}\)</span>.</p></li>
<li><p>Füge alle Nachbarn der Kernpunkte in <span class="math notranslate nohighlight">\(C_i\)</span> dem Cluster hinzu, also <span class="math notranslate nohighlight">\(C_i = \bigcup_{x \in core(C_i)} neighbors(x)\)</span>.</p></li>
<li><p>Wiederhole Schritt 3, bis keine weiteren Instanzen mehr hinzugefügt werden.</p></li>
<li><p>Falls es weitere Kernpunkte gibt, die noch keinem Cluster zugeordnet wurden, setze <span class="math notranslate nohighlight">\(i=i+1\)</span> und gehe zurück zu Schritt 2.</p></li>
</ol>
<p>Die Arbeitsweise des Algorithmus können wir uns am Beispiel verdeutlichen.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">core_samples_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">dbscan</span><span class="o">.</span><span class="n">labels_</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">)</span>
<span class="n">core_samples_mask</span><span class="p">[</span><span class="n">dbscan</span><span class="o">.</span><span class="n">core_sample_indices_</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>

<span class="n">cluster_0_mask</span> <span class="o">=</span> <span class="n">dbscan</span><span class="o">.</span><span class="n">labels_</span><span class="o">==</span><span class="mi">0</span>
<span class="n">cluster_0_core_mask</span> <span class="o">=</span> <span class="n">cluster_0_mask</span> <span class="o">&amp;</span> <span class="n">core_samples_mask</span>
<span class="n">cluster_0_density_connected_mask</span> <span class="o">=</span> <span class="n">cluster_0_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">core_samples_mask</span>

<span class="n">cluster_1_mask</span> <span class="o">=</span> <span class="n">dbscan</span><span class="o">.</span><span class="n">labels_</span><span class="o">==</span><span class="mi">1</span>
<span class="n">cluster_1_core_mask</span> <span class="o">=</span> <span class="n">cluster_1_mask</span> <span class="o">&amp;</span> <span class="n">core_samples_mask</span>
<span class="n">cluster_1_density_connected_mask</span> <span class="o">=</span> <span class="n">cluster_1_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">core_samples_mask</span>

<span class="n">cluster_2_mask</span> <span class="o">=</span> <span class="n">dbscan</span><span class="o">.</span><span class="n">labels_</span><span class="o">==</span><span class="mi">2</span>
<span class="n">cluster_2_core_mask</span> <span class="o">=</span> <span class="n">cluster_2_mask</span> <span class="o">&amp;</span> <span class="n">core_samples_mask</span>
<span class="n">cluster_2_density_connected_mask</span> <span class="o">=</span> <span class="n">cluster_2_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">core_samples_mask</span>

<span class="n">cluster_3_mask</span> <span class="o">=</span> <span class="n">dbscan</span><span class="o">.</span><span class="n">labels_</span><span class="o">==</span><span class="mi">3</span>
<span class="n">cluster_3_core_mask</span> <span class="o">=</span> <span class="n">cluster_3_mask</span> <span class="o">&amp;</span> <span class="n">core_samples_mask</span>
<span class="n">cluster_3_density_connected_mask</span> <span class="o">=</span> <span class="n">cluster_3_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">core_samples_mask</span>

<span class="n">f</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Kernpunkte&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">core_samples_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">core_samples_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;steelblue&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">core_samples_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">core_samples_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;Grey&#39;</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Cluster 1 erstellen&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">cluster_0_core_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">cluster_0_core_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">cluster_0_density_connected_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">cluster_0_density_connected_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">cluster_0_mask</span> <span class="o">&amp;</span> <span class="n">core_samples_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">cluster_0_mask</span> <span class="o">&amp;</span> <span class="n">core_samples_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;steelblue&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">cluster_0_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">core_samples_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">cluster_0_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">core_samples_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;Grey&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Cluster 2 erstellen&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">cluster_0_core_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">cluster_0_core_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">cluster_0_density_connected_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">cluster_0_density_connected_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">cluster_1_core_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">cluster_1_core_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">cluster_1_density_connected_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">cluster_1_density_connected_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">cluster_0_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">cluster_1_mask</span> <span class="o">&amp;</span> <span class="n">core_samples_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">cluster_0_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">cluster_1_mask</span> <span class="o">&amp;</span> <span class="n">core_samples_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;steelblue&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">cluster_0_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">cluster_1_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">core_samples_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">cluster_0_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">cluster_1_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">core_samples_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;Grey&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Cluster 3 erstellen&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">cluster_0_core_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">cluster_0_core_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">cluster_0_density_connected_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">cluster_0_density_connected_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">cluster_1_core_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">cluster_1_core_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">cluster_1_density_connected_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">cluster_1_density_connected_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">cluster_2_core_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">cluster_2_core_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">cluster_2_density_connected_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">cluster_2_density_connected_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">cluster_0_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">cluster_1_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">cluster_2_mask</span> <span class="o">&amp;</span> <span class="n">core_samples_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">cluster_0_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">cluster_1_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">cluster_2_mask</span> <span class="o">&amp;</span> <span class="n">core_samples_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;steelblue&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">cluster_0_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">cluster_1_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">cluster_2_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">core_samples_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">cluster_0_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">cluster_1_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">cluster_2_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">core_samples_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;Grey&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Cluster 4 erstellen&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">cluster_0_core_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">cluster_0_core_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">cluster_0_density_connected_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">cluster_0_density_connected_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">cluster_1_core_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">cluster_1_core_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">cluster_1_density_connected_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">cluster_1_density_connected_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">cluster_2_core_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">cluster_2_core_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">cluster_2_density_connected_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">cluster_2_density_connected_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">cluster_3_core_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">cluster_3_core_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">cluster_3_density_connected_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">cluster_3_density_connected_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">cluster_0_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">cluster_1_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">cluster_2_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">cluster_3_mask</span> <span class="o">&amp;</span> <span class="n">core_samples_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">cluster_0_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">cluster_1_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">cluster_2_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">cluster_3_mask</span> <span class="o">&amp;</span> <span class="n">core_samples_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;steelblue&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">cluster_0_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">cluster_1_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">cluster_2_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">cluster_3_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">core_samples_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">cluster_0_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">cluster_1_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">cluster_2_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">cluster_3_mask</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">core_samples_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;Grey&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_06_21_0.png" src="../_images/kapitel_06_21_0.png" />
</div>
</div>
<p>Im Detail passiert Folgendes:</p>
<ul class="simple">
<li><p>Als Erstes werden alle Kernpunkte bestimmt. Diese Instanzen werden später in jedem Fall zu einem Cluster gehören, da sie sich in einer dichten Nachbarschaft befinden.</p></li>
<li><p>Anschließend wird das erste Cluster erstellt. Hierzu wird ein Kernpunkt ausgewählt, im Beispiel eine Instanz aus dem Bereich rechts oben. Alle Kernpunkte in dieser Region werden zu diesem Cluster hinzugefügt sowie auch alle Nachbarn der Kernpunkte, die nicht selbst Kernpunkte sind. Die mit einem lila Kreuz markierten Instanzen sind selbst keine Kernpunkte, befinden sich aber in der Nachbarschaft eines Kernpunkts und gehören daher dem Cluster an. Dies sind quasi die Häuser an der Stadtgrenze. Es gibt auch einige graue Kreuze in der Region rechts oben. Diese befinden sich nicht in der Nachbarschaft eines Kernpunkts und werden zu Rauschen.</p></li>
<li><p>Anschließend wird ein neuer Kernpunkt gewählt, um das zweite Cluster zu initialisieren. In diesem Fall wurde eine Instanz im unteren Bereich ausgewählt. Es werden wieder alle Nachbarn hinzugefügt, einige Instanzen liegen in keiner Nachbarschaft und bleiben als Rauschen.</p></li>
<li><p>Dies wird noch zwei Mal wiederholt, um die letzten beiden Cluster wachsen zu lassen.</p></li>
<li><p>Da jetzt alle Kernpunkte einem Cluster zugewiesen wurden, terminiert der Algorithmus. Alle Instanzen, die jetzt noch mit einem grauen Kreuz markiert sind, sind Rauschen.</p></li>
</ul>
</div>
<div class="section" id="bestimmen-von-epsilon-und-minpts">
<h3><span class="section-number">6.5.2. </span>Bestimmen von <span class="math notranslate nohighlight">\(\epsilon\)</span> und <span class="math notranslate nohighlight">\(minPts\)</span><a class="headerlink" href="#bestimmen-von-epsilon-und-minpts" title="Permalink to this headline">¶</a></h3>
<p>Ein Vorteil von DBSCAN ist, dass wir die Anzahl der Cluster nicht selbst bestimmen müssen. Stattdessen werden diese vom Algorithmus auf Grundlage der Kernpunkte bestimmt. Wir müssen jedoch definieren, was eine (dichte) Nachbarschaft ist, indem wir die Parameter <span class="math notranslate nohighlight">\(\epsilon\)</span> und <span class="math notranslate nohighlight">\(minPts\)</span> festlegen. Genauso wie ein schlecht gewähltes <span class="math notranslate nohighlight">\(k\)</span> bei den bisherigen Algorithmen zu schlechten Ergebnissen führen kann, gilt dies auch für diese Parameter. In unserem Beispiel haben wir <span class="math notranslate nohighlight">\(\epsilon=0,03\)</span> und <span class="math notranslate nohighlight">\(minPts=4\)</span> verwendet. Hier sind die Ergebnisse für einige andere Werte.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">f</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="n">dbscan</span> <span class="o">=</span> <span class="n">DBSCAN</span><span class="p">(</span><span class="n">eps</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">min_samples</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">dbscan</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">noise_mask</span> <span class="o">=</span> <span class="n">dbscan</span><span class="o">.</span><span class="n">labels_</span><span class="o">&lt;</span><span class="mi">0</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Ergebnis mit $\epsilon=0.01, minPts=4$&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">dbscan</span><span class="o">.</span><span class="n">labels_</span><span class="p">[</span><span class="o">~</span><span class="n">noise_mask</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;Grey&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>

<span class="n">dbscan</span> <span class="o">=</span> <span class="n">DBSCAN</span><span class="p">(</span><span class="n">eps</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">min_samples</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">dbscan</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">noise_mask</span> <span class="o">=</span> <span class="n">dbscan</span><span class="o">.</span><span class="n">labels_</span><span class="o">&lt;</span><span class="mi">0</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Ergebnis mit $\epsilon=0.05$, minPts=4&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">dbscan</span><span class="o">.</span><span class="n">labels_</span><span class="p">[</span><span class="o">~</span><span class="n">noise_mask</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;Grey&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>

<span class="n">dbscan</span> <span class="o">=</span> <span class="n">DBSCAN</span><span class="p">(</span><span class="n">eps</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span> <span class="n">min_samples</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">dbscan</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">noise_mask</span> <span class="o">=</span> <span class="n">dbscan</span><span class="o">.</span><span class="n">labels_</span><span class="o">&lt;</span><span class="mi">0</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Ergebnis mit $\epsilon=0.01, minPts=2$&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">dbscan</span><span class="o">.</span><span class="n">labels_</span><span class="p">[</span><span class="o">~</span><span class="n">noise_mask</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;Grey&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>

<span class="n">dbscan</span> <span class="o">=</span> <span class="n">DBSCAN</span><span class="p">(</span><span class="n">eps</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">min_samples</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">dbscan</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">noise_mask</span> <span class="o">=</span> <span class="n">dbscan</span><span class="o">.</span><span class="n">labels_</span><span class="o">&lt;</span><span class="mi">0</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Ergebnis mit $\epsilon=0.05$, minPts=20&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">dbscan</span><span class="o">.</span><span class="n">labels_</span><span class="p">[</span><span class="o">~</span><span class="n">noise_mask</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;Grey&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_06_23_0.png" src="../_images/kapitel_06_23_0.png" />
</div>
</div>
<p>In der ersten Spalte sehen wir Ergebnisse für sehr kleine Werte von <span class="math notranslate nohighlight">\(\epsilon\)</span> mit 0,01. Wenn man die <span class="math notranslate nohighlight">\(minPts\)</span> bei 4 belässt, findet man keine echten Cluster mehr. Es gibt zwar einige wenige Instanzen, die noch als Kernpunkte erkannt werden, die meisten Instanzen sind jedoch Rauschen. Wenn man <span class="math notranslate nohighlight">\(minPts\)</span> auf 2 reduziert, wird das Ergebnis besser. Es gibt vier größere Cluster, die ähnlich zu unserem Beispielergebnis weiter oben sind. Es gibt jedoch noch viele sehr kleine Cluster, die nur drei Instanzen beinhalten. Das ist nicht wünschenswert: Diese kleinen Cluster sollten entweder zu größeren Clustern gehören oder als Rauschen erkannt werden. In der zweiten Spalte sehen wir, was passiert, wenn wir ein relativ großes <span class="math notranslate nohighlight">\(\epsilon\)</span>von 0,05 wählen. Die Ergebnisse sehen besser aus als mit dem kleinen Epsilon. Wenn wir die <span class="math notranslate nohighlight">\(minPts\)</span> bei 4 belassen, finden wir drei große Cluster, die dem Ergebnis vom <span class="math notranslate nohighlight">\(k\)</span>-Means- und EM-Clustering mit drei Clustern entsprechen. Wenn wir die <span class="math notranslate nohighlight">\(minPts\)</span> auf 20 erhöhen, haben wir weniger Kernpunkte, die stattdessen große Nachbarschaften aufweisen. Das führt zu einem ähnlichen Ergebnis wie mit unserer ursprünglichen Parameterwahl von <span class="math notranslate nohighlight">\(\epsilon=0,03\)</span> und <span class="math notranslate nohighlight">\(minPts=4\)</span>, jedoch mit weniger Rauschen.</p>
<p>Im Allgemeinen ist es so, dass man mit einem kleinen <span class="math notranslate nohighlight">\(\epsilon\)</span> die Wahrscheinlichkeit für viel Rauschen oder sehr kleine Cluster erhöht. Wenn man ein größeres  <span class="math notranslate nohighlight">\(\epsilon\)</span> wählt, führt dies dazu, dass Cluster verschmolzen werden. Wenn man <span class="math notranslate nohighlight">\(minPts\)</span> reduziert, bekommt man mehr Kernpunkte, was dazu führt, dass Brücken zwischen Clustern entstehen und diese zusammengeführt werden oder auch dass sehr kleine Cluster möglich werden. Wenn man <span class="math notranslate nohighlight">\(minPts\)</span> erhöht, kann man kleine Cluster vermeiden, es könnte aber auch viel Rauschen geben, wenn <span class="math notranslate nohighlight">\(\epsilon\)</span> zu klein gewählt wird. Das Zusammenspiel der Parameter ist also relativ komplex und es gibt keine einfache Faustregel, um gute Werte zu bestimmen.</p>
<p>Es gibt jedoch ein analytisches Werkzeug, mit dessen Hilfe man gute Kombinationen von <span class="math notranslate nohighlight">\(\epsilon\)</span> und <span class="math notranslate nohighlight">\(minPts\)</span> ermitteln kann. Hierzu nutzt man den Abstand des <span class="math notranslate nohighlight">\(k\)</span><em>-ten nächsten Nachbarn</em> (engl. <span class="math notranslate nohighlight">\(k\)</span>-th <em>nearest neighbor</em>) einer Instanz, um zu sehen, wie viele Kernpunkte es bei einer Parameterkombination gibt.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">NearestNeighbors</span>

<span class="c1"># we use sklearn to find the nearest neighbors</span>
<span class="n">neigh</span> <span class="o">=</span> <span class="n">NearestNeighbors</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">nbrs</span> <span class="o">=</span> <span class="n">neigh</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">distances</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">nbrs</span><span class="o">.</span><span class="n">kneighbors</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">):</span>
    <span class="n">distances_k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">distances</span><span class="p">[:,</span><span class="n">k</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">distances_k</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;minPts=</span><span class="si">%i</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;epsilon&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Anzahl Kernpunkte&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_06_25_0.png" src="../_images/kapitel_06_25_0.png" />
</div>
</div>
<p>Gute Parameter findet man in dem Bereich, in dem die Kurve stark gekrümmt ist. Für die Kurve von <span class="math notranslate nohighlight">\(minPts=4\)</span> ist das zum Beispiel im Bereich zwischen 0,03 und 0,04 <span class="math notranslate nohighlight">\(\epsilon\)</span> der Fall.</p>
</div>
<div class="section" id="probleme-bei-dbscan">
<h3><span class="section-number">6.5.3. </span>Probleme bei DBSCAN<a class="headerlink" href="#probleme-bei-dbscan" title="Permalink to this headline">¶</a></h3>
<p>Bei DBSCAN muss man weder die Anzahl der Cluster auswählen, noch gibt es Einschränkungen bezüglich der Form der Cluster. Trotzdem gibt es auch bei diesem Algorithmus verschiedene Probleme, die man berücksichtigen sollte.</p>
<ul class="simple">
<li><p>Das größte Problem ist die Wahl von <span class="math notranslate nohighlight">\(\epsilon\)</span> und <span class="math notranslate nohighlight">\(minPts\)</span>. Insbesondere wenn es Gruppen gibt, die sehr nah beieinander liegen, oder die Daten insgesamt sehr dicht sind, kann es schwierig sein, sinnvolle Parameter und Cluster zu finden. Der Grund dafür liegt darin, dass in diesem Fall einige Datenpunkte zu <em>Brückenpunkten</em> (engl. <em>bridge points</em>) zwischen zwei Clustern werden können. Man spricht von Brückenpunkten, wenn es Kernpunkte gibt, die dafür sorgen, dass zwei Cluster verschmelzen. Um dies zu verhindern, muss man große Werte für  wählen, damit es keine Kernpunkte in der Grenzregion eines Clusters gibt. Dies führt aber auch dazu, dass man <span class="math notranslate nohighlight">\(\epsilon\)</span> relativ groß wählen muss, was wieder zu neuen Problemen führen kann.</p></li>
<li><p>Ein weiteres Problem sind Gruppierungen mit unterschiedlichen Dichten. Im folgenden Beispiel sollen zwei kreisförmige Gruppen geclustert werden. Mit 100 Instanzen pro Kreis sind die Daten des äußeren Kreises aufgrund des größeren Umfangs nicht sehr dicht im Verhältnis zum inneren Kreis. Daher gibt es einige Stellen ohne Kernpunkte, sodass mehrere kleinere Cluster im äußeren Kreis entstehen. Wenn man die Anzahl der Instanzen erhöht, werden beide Kreise korrekt geclustert.</p></li>
</ul>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_circles</span>

<span class="n">f</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="n">X</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">make_circles</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">dbscan</span> <span class="o">=</span> <span class="n">DBSCAN</span><span class="p">(</span><span class="n">eps</span><span class="o">=</span><span class="mf">0.15</span><span class="p">,</span> <span class="n">min_samples</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">dbscan</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">noise_mask</span> <span class="o">=</span> <span class="n">dbscan</span><span class="o">.</span><span class="n">labels_</span><span class="o">&lt;</span><span class="mi">0</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Zwei Kreise mit 100 Instanzen&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">dbscan</span><span class="o">.</span><span class="n">labels_</span><span class="p">[</span><span class="o">~</span><span class="n">noise_mask</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;Grey&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>

<span class="n">X</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">make_circles</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">dbscan</span> <span class="o">=</span> <span class="n">DBSCAN</span><span class="p">(</span><span class="n">eps</span><span class="o">=</span><span class="mf">0.15</span><span class="p">,</span> <span class="n">min_samples</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">dbscan</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">noise_mask</span> <span class="o">=</span> <span class="n">dbscan</span><span class="o">.</span><span class="n">labels_</span><span class="o">&lt;</span><span class="mi">0</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Zwei Kreise mit 200 Instanzen&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">dbscan</span><span class="o">.</span><span class="n">labels_</span><span class="p">[</span><span class="o">~</span><span class="n">noise_mask</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;Grey&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_06_27_0.png" src="../_images/kapitel_06_27_0.png" />
</div>
</div>
<ul class="simple">
<li><p>Da die Dichte der Daten wichtig für die Wahl von <span class="math notranslate nohighlight">\(\epsilon\)</span> und <span class="math notranslate nohighlight">\(minPts\)</span> ist, folgt auch, dass man DBSCAN nicht ohne Weiteres auf eine zufällige Teilmenge der Daten anwenden kann, ohne die Parameter anzupassen. Der Grund hierfür ist, dass eine Teilmenge in der Regel eine geringere Dichte hat.</p></li>
<li><p>Da DBSCAN ähnlich wie <span class="math notranslate nohighlight">\(k\)</span>-Means die Cluster basierend auf Distanzen bestimmt, ist der Algorithmus ebenfalls anfällig für Skaleneffekte, wenn Merkmale verschiedene Größenordnungen haben.</p></li>
</ul>
</div>
</div>
<div class="section" id="single-linkage-clustering">
<h2><span class="section-number">6.6. </span>Single Linkage Clustering<a class="headerlink" href="#single-linkage-clustering" title="Permalink to this headline">¶</a></h2>
<p>Ein wichtiger Mechanismus, wie Städte wachsen, ist der Zusammenschluss mit anderen Städten und Gemeinden. Dies passiert natürlich nicht beliebig, sondern mit Städten, die ohnehin sehr nah beieinander liegen. Das ist die Grundidee vom <em>hierarchischen Clustern</em>. Die Idee ist ähnlich wie bei DBSCAN, nur dass wir nicht mit dichten Nachbarschaften arbeiten, sondern stattdessen kleine Cluster zu größeren Clustern zusammenfügen. Als Ergebnis hat man daher sehr viele verschiedene mögliche Clusterergebnisse, da das Zusammenfügen durchgeführt wird, bis alle Instanzen zu einem einzigen Cluster verschmolzen sind. Anschließend wählt man von diesen vielen Ergebnissen ein passendes aus. Das Endergebnis sieht dann zum Beispiel wie folgt aus.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib</span> <span class="k">as</span> <span class="nn">mpl</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">AgglomerativeClustering</span>
<span class="kn">from</span> <span class="nn">scipy.cluster.hierarchy</span> <span class="kn">import</span> <span class="n">dendrogram</span>

<span class="k">def</span> <span class="nf">warn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">pass</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">warn</span> <span class="o">=</span> <span class="n">warn</span>

<span class="n">eps</span> <span class="o">=</span> <span class="mf">0.03</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y_true</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">center_box</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span>
                       <span class="n">cluster_std</span><span class="o">=</span><span class="mf">0.04</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">sl</span> <span class="o">=</span> <span class="n">AgglomerativeClustering</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">distance_threshold</span><span class="o">=</span><span class="mf">0.03</span><span class="p">,</span><span class="n">linkage</span><span class="o">=</span><span class="s1">&#39;single&#39;</span><span class="p">,</span> <span class="n">compute_full_tree</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">sl</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">y_sl</span> <span class="o">=</span> <span class="n">sl</span><span class="o">.</span><span class="n">labels_</span>

<span class="n">f</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Daten (ohne Clustering)&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">);</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Ergebnis des Single Linkage Clustering&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y_sl</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_06_29_0.png" src="../_images/kapitel_06_29_0.png" />
</div>
</div>
<p>Das Ergebnis sieht ähnlich aus wie bei DBSCAN, es gibt jedoch einen großen Unterschied: Es entsteht kein Rauschen, stattdessen kommt es zu einigen sehr kleinen Clustern, die zum Teil sogar nur aus einer Instanz bestehen, zum Beispiel am linken Rand.</p>
<p>Es gibt verschiedene hierarchische Clusteralgorithmen, die sich darin unterscheiden, wie die Cluster zusammengefügt werden. Darunter sind auch Algorithmen, die genau andersrum vorgehen und ein großes Cluster schrittweise in kleine Cluster zerlegen, zum Beispiel das <em>Complete Linkage Clustering</em>. Im Folgenden betrachten wir das <em>Single Linkage Clustering</em> (SLINK) als Beispiel für einen hierarchischen Clusteralgorithmus.</p>
<div class="section" id="der-slink-algorithmus">
<h3><span class="section-number">6.6.1. </span>Der SLINK-Algorithmus<a class="headerlink" href="#der-slink-algorithmus" title="Permalink to this headline">¶</a></h3>
<p>SLINK ist ein iterativer Algorithmus, der in jedem Schritt die zwei Cluster mit dem geringsten Abstand voneinander zu einem neuen Cluster zusammenfügt. Zu Beginn ist jede Instanz in einem eigenen Cluster. SLINK berechnet dann die Distanzen für zwei Cluster <span class="math notranslate nohighlight">\(C\)</span> und <span class="math notranslate nohighlight">\(C'\)</span> als die Distanz zwischen den Instanzen der jeweiligen Cluster, die sich am nächsten liegen, also</p>
<div class="math notranslate nohighlight">
\[d(C, C') = \min_{x\in C, x' \in X'} dist(x, x').\]</div>
<p>Für jedes Cluster, das beim Zusammenfügen erstellt wird, berechnet SLINK ein <em>Level</em>. Das Level gibt an, wie weit die Cluster, die verbunden wurden, voneinander getrennt waren. Man kann sich das als den Abstand zwischen zwei Städten, die zu einer Stadt verbunden werden, vorstellen. Ein niedriges Level heißt also, dass zwei Cluster, die verbunden wurden, nah beieinander lagen, ein hohes Level heißt, dass es eine größere Lücke zwischen den Clustern gab. Diese Lücke ist dann automatisch auch Teil des größeren Clusters.</p>
<p>Basierend auf dieser Idee ist der eigentlich Algorithmus relativ einfach:</p>
<ol class="simple">
<li><p>Initialisiere die Basiscluster <span class="math notranslate nohighlight">\(C = \{x\}\)</span> für alle <span class="math notranslate nohighlight">\(x \in X\)</span> mit Level <span class="math notranslate nohighlight">\(L(C) = 0\)</span>.</p></li>
<li><p>Finde die zwei Cluster, deren Abstand minimal ist, also <span class="math notranslate nohighlight">\(C, C' = \arg\min_{C, C'} d(C,C')\)</span>.</p></li>
<li><p>Verbinde <span class="math notranslate nohighlight">\(C\)</span> und <span class="math notranslate nohighlight">\(C'\)</span> zu einem neuen Cluster <span class="math notranslate nohighlight">\(C_{new} = C \cup C'\)</span> mit Level <span class="math notranslate nohighlight">\(L(C_{new}) = d(C, C')\)</span>.</p></li>
<li><p>Wiederhole die Schritte 2 und 3, bis alle Instanzen in einem Cluster sind.</p></li>
</ol>
<p>Das Ergebnis dieses Algorithmus ist also kein eindeutiges Clustering, sondern sehr viele mögliche Ergebnisse. Diese kann man sich am besten durch ein <em>Dendrogramm</em> veranschaulichen.</p>
</div>
<div class="section" id="dendrogramme">
<h3><span class="section-number">6.6.2. </span>Dendrogramme<a class="headerlink" href="#dendrogramme" title="Permalink to this headline">¶</a></h3>
<p>Ein <em>Dendrogramm</em> ist die visuelle Darstellung einer Baumstruktur als Graph. Für unser Beispiel bekommen wir folgendes Dendrogramm.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.cluster.hierarchy</span> <span class="kn">import</span> <span class="n">dendrogram</span>

<span class="k">def</span> <span class="nf">plot_dendrogram</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="c1"># create linkage matrix and then plot the dendrogram</span>

    <span class="c1"># create the counts of samples under each node</span>
    <span class="n">counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">children_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">n_samples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">labels_</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">merge</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">children_</span><span class="p">):</span>
        <span class="n">current_count</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">child_idx</span> <span class="ow">in</span> <span class="n">merge</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">child_idx</span> <span class="o">&lt;</span> <span class="n">n_samples</span><span class="p">:</span>
                <span class="n">current_count</span> <span class="o">+=</span> <span class="mi">1</span>  <span class="c1"># leaf node</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">current_count</span> <span class="o">+=</span> <span class="n">counts</span><span class="p">[</span><span class="n">child_idx</span> <span class="o">-</span> <span class="n">n_samples</span><span class="p">]</span>
        <span class="n">counts</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">current_count</span>

    <span class="n">linkage_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="n">model</span><span class="o">.</span><span class="n">children_</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">distances_</span><span class="p">,</span>
                                      <span class="n">counts</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>

    <span class="c1"># plot the corresponding dendrogram</span>
    <span class="n">dendrogram</span><span class="p">(</span><span class="n">linkage_matrix</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="n">f</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Dendrogramm vom Single Linkage Clustering&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Level&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Instanzen&quot;</span><span class="p">)</span>
<span class="n">plot_dendrogram</span><span class="p">(</span><span class="n">sl</span><span class="p">,</span><span class="n">color_threshold</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span> <span class="n">no_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">above_threshold_color</span><span class="o">=</span><span class="s1">&#39;grey&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Gekürztes Dendrogramm mit nur den obersten 15 Leveln&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Level&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Instanzen&quot;</span><span class="p">)</span>
<span class="n">plot_dendrogram</span><span class="p">(</span><span class="n">sl</span><span class="p">,</span><span class="n">color_threshold</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span> <span class="n">no_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">above_threshold_color</span><span class="o">=</span><span class="s1">&#39;grey&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">truncate_mode</span><span class="o">=</span><span class="s1">&#39;level&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_06_31_0.png" src="../_images/kapitel_06_31_0.png" />
</div>
</div>
<p>Am unteren Rand des Dendrogramms sind unsere Instanzen, was gleichbedeutend ist mit den Clustern mit Level null. Jede horizontale Linie verbindet zwei Cluster zu einem neuen Cluster. Die Lage dieser Linie auf der y-Achse ist das Level des neuen Clusters, das durch die Verbindung entsteht. Die horizontale Linie bei 0,1 verbindet zum Beispiel alle Instanzen in den Clustern des linken Teilbaums mit den Instanzen im rechten Teilbaum zu einem neuen Cluster mit Level 0,1. Aufgrund der Definition des Levels wissen wir, dass alle Instanzen im linken Teilbaum mindestens eine Distanz von 0,1 zu allen Instanzen im rechten Teilbaum haben. Außerdem ist noch eine schwarze horizontale Linie bei 0,03 eingezeichnet. Diese Linie markiert die Clusterauswahl, für die wir uns im Beispiel weiter oben entschieden haben.</p>
<p>Die linke Grafik ist sehr dicht am unteren Rand, sodass man eigentlich nur noch größere gleichfarbige Regionen erkennt. Dies liegt daran, dass jede einzelne Instanz eine kleine Linie am unteren Rand darstellt, in unserem Fall gibt es daher 300 solche Linien. Ein einfacher Trick, mit dem man auch den unteren Bereich eines Dendrogramms besser darstellen kann, ist es, die unteren Ebenen des Baums wegzulassen. Man zeigt also nicht die Cluster aller Level, sondern nur die Cluster der höchsten Level. Die rechte Grafik stellt zum Beispiel nur die obersten 15 Level dar. Hierdurch sieht man zwar die Instanzen am unteren Rand nicht mehr, kann aber dafür die Abstände zwischen den kleineren Clustern in diesem Bereich besser erkennen.</p>
<p>Auch wenn Dendrogramme als Visualisierung zunächst etwas gewöhnungsbedürftig sind, handelt es sich um ein hervorragendes Werkzeug, um visuell geeignete Cluster auszuwählen. Im obigen Beispiel haben wir einen festen Grenzwert von 0,03 verwendet, um die Cluster anhand der Level auszuwählen. Im Dendrogramm kann man direkt erkennen, was zum Beispiel passieren würde, wenn wir stattdessen einen Grenzwert von 0,05 wählen würden: Das grüne und das braune Cluster wären dann zusammen in einem Cluster. Wenn man den Grenzwert zwischen 0,06 und 0,26 wählen würde, hätte man zwei Cluster. Für Werte zwischen 0,06 und 0,1 hätte man eigentlich auch schon nur noch zwei große Cluster, es gäbe jedoch formal noch ein weiteres Cluster mit einer einzigen Instanz. Insgesamt erkennt man die Auswirkungen des Grenzwerts ,den man wählt, deutlich besser als beim ähnlichen DBSCAN-Algorithmus, wo die Auswirkungen unterschiedlicher Werte von <span class="math notranslate nohighlight">\(\epsilon\)</span> und <span class="math notranslate nohighlight">\(minPts\)</span> nicht einfach vorherzusagen sind.</p>
</div>
<div class="section" id="probleme-bei-slink">
<h3><span class="section-number">6.6.3. </span>Probleme bei SLINK<a class="headerlink" href="#probleme-bei-slink" title="Permalink to this headline">¶</a></h3>
<p>Wie alle Algorithmen, ist auch SLINK nicht ohne Probleme.</p>
<ul class="simple">
<li><p>Das größte Problem bei SLINK (und beim hierarchischen Clustern im Allgemeinen) ist die Skalierbarkeit für größere Datensätze. Da die Algorithmen in der Regel erfordern, dass man eine quadratische Distanzmatrix berechnet, in der die Distanzen zwischen allen Instanzen gespeichert werden, ist insbesondere der Speicherbedarf problematisch. Bei 100.000 Instanzen wären dies zum Beispiel bereits etwa 4 Gigabyte, selbst wenn man die Symmetrie ausnutzt und nur das obere Dreieck der Distanzmatrix speichert. Daher können hierarchische Clusteralgorithmen in der Regel nur bei kleinen Datensätzen angewandt werden.</p></li>
<li><p>Ebenso wie beim DBSCAN-Algorithmus sind Bereiche mit unterschiedlichen Dichten problematisch. Es ist jedoch einfacher, einen guten Grenzwert zu bestimmen, mit dem man die Cluster trennen kann. Hier hilft das Dendrogramm. Im Beispiel mit den Kreisen erkennt man zum Beispiel, dass man für Grenzwerte zwischen 0,2 und 0,4 das richtige Ergebnis findet.</p></li>
</ul>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_circles</span>

<span class="n">X</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">make_circles</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">eps</span><span class="o">=</span><span class="mf">0.2</span>

<span class="n">sl</span> <span class="o">=</span> <span class="n">AgglomerativeClustering</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">distance_threshold</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span><span class="n">linkage</span><span class="o">=</span><span class="s1">&#39;single&#39;</span><span class="p">,</span> <span class="n">compute_full_tree</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">sl</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">y_sl</span> <span class="o">=</span> <span class="n">sl</span><span class="o">.</span><span class="n">labels_</span>

<span class="n">f</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Ergebnis vom Single Linkage Clustering&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y_sl</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Dendrogramm vom Single Linkage Clustering&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Level&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Objects&#39;</span><span class="p">)</span>
<span class="n">plot_dendrogram</span><span class="p">(</span><span class="n">sl</span><span class="p">,</span><span class="n">color_threshold</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span> <span class="n">no_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">above_threshold_color</span><span class="o">=</span><span class="s1">&#39;grey&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_06_33_0.png" src="../_images/kapitel_06_33_0.png" />
</div>
</div>
<ul class="simple">
<li><p>Ein weiteres Problem, das wir bereits von DBSCAN kennen, sind Brückenpunkte, die Cluster verbinden. Der Grund dafür ist, dass DBSCAN und SLINK die gleichen Cluster liefern, wenn man <span class="math notranslate nohighlight">\(minPts=1\)</span> setzt. Der Grenzwert für die Level ist dann von der Bedeutung identisch zum <span class="math notranslate nohighlight">\(\epsilon\)</span> von DBSCAN. Brückenpunkte sind bei SLINK sogar ein noch größeres Problem. Während man bei DBSCAN durch einen hohen Wert für minPts versuchen kann, zu verhindern, dass die Brückenpunkte zu Kernpunkten werden, ist dies bei SLINK nicht möglich.</p></li>
<li><p>SLINK kann zu vielen sehr kleinen Clustern führen, da jede Instanz initial einem eigenen Cluster zugewiesen wird, und sich dies, je nach Grenzwert, möglicherweise auch im Endergebnis nicht ändert. Ausreißer führen daher dazu, dass es viele Cluster gibt, die irrelevant sind, aber die Analyse erschweren.</p></li>
<li><p>Da SLINK auch distanzbasiert ist, ist der Algorithmus weiter anfällig für Skaleneffekte durch unterschiedliche Wertebereiche, wie wir es beim <span class="math notranslate nohighlight">\(k\)</span>-Means-Algorithmus beschrieben haben.</p></li>
</ul>
</div>
</div>
<div class="section" id="vergleich-der-algorithmen">
<h2><span class="section-number">6.7. </span>Vergleich der Algorithmen<a class="headerlink" href="#vergleich-der-algorithmen" title="Permalink to this headline">¶</a></h2>
<p>Während der Beschreibung haben wir immer wieder einzelne Aspekte der Algorithmen miteinander verglichen, insbesondere wenn es ähnliche Probleme gab oder wenn ein Algorithmus ein Problem geschickt umgangen hat. Im letzten Teil dieses Kapitels wollen wir verschiedene Eigenschaften der Algorithmen direkt miteinander vergleichen.</p>
<div class="section" id="clusterformen">
<h3><span class="section-number">6.7.1. </span>Clusterformen<a class="headerlink" href="#clusterformen" title="Permalink to this headline">¶</a></h3>
<p>Die wohl wichtigste Eigenschaft der Algorithmen ist es, Cluster richtig zu erkennen. Hierbei spielt insbesondere die Form der Cluster eine Rolle, wie wir schon am Beispiel der Kreise und der Halbmonde gesehen haben.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib</span> <span class="k">as</span> <span class="nn">mpl</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span><span class="p">,</span> <span class="n">make_moons</span><span class="p">,</span> <span class="n">make_circles</span>
<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">DBSCAN</span><span class="p">,</span> <span class="n">KMeans</span><span class="p">,</span> <span class="n">AgglomerativeClustering</span>
<span class="kn">from</span> <span class="nn">sklearn.mixture</span> <span class="kn">import</span> <span class="n">GaussianMixture</span>
<span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">ListedColormap</span>

<span class="n">datasets</span> <span class="o">=</span> <span class="p">[</span><span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">center_box</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">cluster_std</span><span class="o">=</span><span class="mf">0.04</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">make_moons</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
            <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">make_moons</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[[</span><span class="mf">0.3</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mf">0.35</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.4</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.45</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.55</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.6</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.65</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.7</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.75</span><span class="p">,</span><span class="mi">0</span><span class="p">]],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span><span class="kc">None</span><span class="p">),</span>
            <span class="n">make_circles</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)]</span>

<span class="c1"># parameter values for the different data sets</span>
<span class="n">k</span> <span class="o">=</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span>
<span class="n">eps</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.03</span><span class="p">,</span><span class="mf">0.15</span><span class="p">,</span><span class="mf">0.15</span><span class="p">,</span><span class="mf">0.15</span><span class="p">]</span>

<span class="n">f</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span> <span class="mi">16</span><span class="p">))</span>

<span class="k">for</span> <span class="n">ds_cnt</span><span class="p">,</span> <span class="n">ds</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">datasets</span><span class="p">):</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">ds</span>
    
    <span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;indigo&#39;</span><span class="p">,</span> <span class="s1">&#39;gold&#39;</span><span class="p">,</span> <span class="s1">&#39;mediumseagreen&#39;</span><span class="p">,</span> <span class="s1">&#39;crimson&#39;</span><span class="p">]</span>
    
    <span class="n">cmap</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">(</span><span class="n">colors</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;cmap_points&quot;</span><span class="p">)</span>
    <span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;indigo&#39;</span><span class="p">,</span> <span class="s1">&#39;crimson&#39;</span><span class="p">,</span> <span class="s1">&#39;mediumseagreen&#39;</span><span class="p">,</span> <span class="s1">&#39;gold&#39;</span><span class="p">]</span>
    
    <span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">k</span><span class="p">[</span><span class="n">ds_cnt</span><span class="p">],</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">kmeans</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">y_kmeans</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">centers</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">cluster_centers_</span>
    
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="n">ds_cnt</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;$k$-Means&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y_kmeans</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">centers</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">centers</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">);</span>
    
    <span class="n">dbscan</span> <span class="o">=</span> <span class="n">DBSCAN</span><span class="p">(</span><span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">[</span><span class="n">ds_cnt</span><span class="p">],</span> <span class="n">min_samples</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
    <span class="n">dbscan</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">noise_mask</span> <span class="o">=</span> <span class="n">dbscan</span><span class="o">.</span><span class="n">labels_</span><span class="o">&lt;</span><span class="mi">0</span>
    
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="n">ds_cnt</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;DBSCAN&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">dbscan</span><span class="o">.</span><span class="n">labels_</span><span class="p">[</span><span class="o">~</span><span class="n">noise_mask</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">noise_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;Grey&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
    
    <span class="n">em</span> <span class="o">=</span> <span class="n">GaussianMixture</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">k</span><span class="p">[</span><span class="n">ds_cnt</span><span class="p">],</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">em</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">y_em</span> <span class="o">=</span> <span class="n">em</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">centers</span> <span class="o">=</span> <span class="n">em</span><span class="o">.</span><span class="n">means_</span>
    
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="n">ds_cnt</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;EM-Clustering&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="p">[</span><span class="n">colors</span><span class="p">[</span><span class="n">y</span><span class="p">]</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">y_em</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">centers</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">centers</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">);</span>
    <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">color</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">colors</span><span class="p">[:</span><span class="n">k</span><span class="p">[</span><span class="n">ds_cnt</span><span class="p">]]):</span>
        <span class="n">v</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigh</span><span class="p">(</span><span class="n">em</span><span class="o">.</span><span class="n">covariances_</span><span class="p">[</span><span class="n">n</span><span class="p">][:</span><span class="mi">2</span><span class="p">,:</span><span class="mi">2</span><span class="p">])</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">angle</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arctan2</span><span class="p">(</span><span class="n">u</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">u</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">angle</span> <span class="o">=</span> <span class="mi">180</span> <span class="o">*</span> <span class="n">angle</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span>  <span class="c1"># convert to degrees</span>
        <span class="n">v</span> <span class="o">=</span> <span class="mf">2.</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
        <span class="n">ell</span> <span class="o">=</span> <span class="n">mpl</span><span class="o">.</span><span class="n">patches</span><span class="o">.</span><span class="n">Ellipse</span><span class="p">(</span><span class="n">em</span><span class="o">.</span><span class="n">means_</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="p">:</span><span class="mi">2</span><span class="p">],</span> <span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                                  <span class="mi">180</span> <span class="o">+</span> <span class="n">angle</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">)</span>
        <span class="n">ell</span><span class="o">.</span><span class="n">set_clip_box</span><span class="p">(</span><span class="n">ax</span><span class="o">.</span><span class="n">bbox</span><span class="p">)</span>
        <span class="n">ell</span><span class="o">.</span><span class="n">set_alpha</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">add_artist</span><span class="p">(</span><span class="n">ell</span><span class="p">)</span>
        
    <span class="n">sl</span> <span class="o">=</span> <span class="n">AgglomerativeClustering</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">distance_threshold</span><span class="o">=</span><span class="n">eps</span><span class="p">[</span><span class="n">ds_cnt</span><span class="p">],</span><span class="n">linkage</span><span class="o">=</span><span class="s1">&#39;single&#39;</span><span class="p">,</span> <span class="n">compute_full_tree</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">sl</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">y_sl</span> <span class="o">=</span> <span class="n">sl</span><span class="o">.</span><span class="n">labels_</span>

    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="n">ds_cnt</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;SLINK&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y_sl</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_06_35_0.png" src="../_images/kapitel_06_35_0.png" />
</div>
</div>
<p>Wie man sieht, liefern alle Algorithmen auf unserem Beispieldatensatz gute Ergebnisse. Im Detail erkennt man jedoch auch hier Unterschiede, insbesondere dort, wo die Cluster nah beieinander liegen. An der Grenze zwischen den beiden Clustern im mittleren Bereich sieht man eine scheinbar zufällige Trennung bei <span class="math notranslate nohighlight">\(k\)</span>-Means und EM Clustering, Rauschen bei DBSCAN, und einige kleine Cluster bei SLINK.</p>
<p>In der zweiten Zeile sehen wir noch einmal die Halbmonde. Hier erkennt man gut, was wir oben bereits beschrieben haben: <span class="math notranslate nohighlight">\(k\)</span>-Means und EM- Clustering sind für solche geometrische Formen nicht geeignet, da sie von einer eher runden bzw. ellipsoiden Form ausgehen. Da DBSCAN und SLINK stattdessen die direkten Nachbarschaften zwischen den Instanzen betrachten, sind solche Daten für diese Algorithmen kein Problem. Im Allgemeinen funktionieren diese Algorithmen mit beliebigen geometrischen Formen, wichtig ist nur, dass es Lücken zwischen den Clustern gibt. Dass dies ein Problem sein kann, zeigt die dritte Reihe. Hier haben wir einige Brückenpunkte hinzugefügt, sodass die Algorithmen nicht mehr zwischen den Clustern unterscheiden können.</p>
<p>Die vierte Reihe zeigt, dass auch runde Clusterformen bei <span class="math notranslate nohighlight">\(k\)</span>-Means und EM-Clustering möglicherweise zu Problemen führen, nämlich dann, wenn es sich um ineinander liegende Kreise handelt.</p>
</div>
<div class="section" id="anzahl-der-cluster">
<h3><span class="section-number">6.7.2. </span>Anzahl der Cluster<a class="headerlink" href="#anzahl-der-cluster" title="Permalink to this headline">¶</a></h3>
<p>Eine weitere Stärke von DBSCAN und SLINK besteht darin, dass man die Anzahl der Cluster nicht vorgeben muss, sondern sich diese automatisch aufgrund der Dichte der Daten und der gewählten Parameter ergibt. Beim EM-Clustering gibt es mit dem BIC ein analytisches Kriterium, mit dem man das Optimum bestimmen kann. Bei <span class="math notranslate nohighlight">\(k\)</span>-Means ist eine manuelle Analyse zwingend erforderlich, zum Beispiel mit der WSS. Hier gibt es kein hartes Kriterium, anhand dessen man einen guten Wert für <span class="math notranslate nohighlight">\(k\)</span> aus dem Liniendiagramm der WSS ablesen kann, stattdessen ist die Erfahrung gefragt, um relevante Änderungen in der Steigung zu erkennen.</p>
</div>
<div class="section" id="ausfuhrungszeit">
<h3><span class="section-number">6.7.3. </span>Ausführungszeit<a class="headerlink" href="#ausfuhrungszeit" title="Permalink to this headline">¶</a></h3>
<p>Die Ausführungszeit kann, je nach Größe des Datensatzes und Anwendungsfall, ein entscheidendes Kriterium bei der Wahl des Algorithmus sein. Unten sieht man die Ergebnisse der Ausführungszeit für das Clustern von Daten mit 20 Merkmalen für 10.000, 100.000, 1.000.000, 10.000.000 und 100.000.000 Instanzen, also für bis zu 15 Gigabyte Daten. SLINK fehlt im Vergleich, da dieser Algorithmus bereits bei 100.000 Instanzen aus den oben beschriebenen Problemen mit dem Platzbedarf nicht mehr korrekt arbeitet. Die Zeiten wurden mit einem normalen Laptop (Intel Core i7-8850 &#64; 2.60GHz, 32 GB RAM) mit scikit-learn 0.24 ermittelt.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">DBSCAN</span><span class="p">,</span> <span class="n">KMeans</span>
<span class="kn">from</span> <span class="nn">sklearn.mixture</span> <span class="kn">import</span> <span class="n">GaussianMixture</span>

<span class="kn">import</span> <span class="nn">timeit</span>

<span class="n">names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;$k$-Means&quot;</span><span class="p">,</span>
         <span class="s2">&quot;EM&quot;</span><span class="p">,</span>
         <span class="s2">&quot;DBSCAN&quot;</span><span class="p">]</span>

<span class="n">clusters</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
    <span class="n">GaussianMixture</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">warm_start</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
    <span class="n">DBSCAN</span><span class="p">(</span><span class="n">eps</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">min_samples</span><span class="o">=</span><span class="mi">4</span><span class="p">)]</span>

<span class="n">instances</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1e3</span><span class="p">,</span><span class="mf">1e4</span><span class="p">,</span><span class="mf">1e5</span><span class="p">,</span><span class="mf">1e6</span><span class="p">,</span><span class="mf">1e7</span><span class="p">]</span>
<span class="n">runtime</span> <span class="o">=</span> <span class="p">[[],[],[]]</span>
<span class="k">for</span> <span class="n">num_instances</span> <span class="ow">in</span> <span class="n">instances</span><span class="p">:</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">num_instances</span><span class="p">),</span> <span class="n">centers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">center_box</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">cluster_std</span><span class="o">=</span><span class="mf">0.04</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">cl_cnt</span><span class="p">,</span> <span class="n">cl</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">clusters</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">cl_cnt</span><span class="o">==</span><span class="mi">2</span> <span class="ow">and</span> <span class="n">num_instances</span><span class="o">&gt;</span><span class="mi">100000</span><span class="p">:</span>
            <span class="c1"># we skip DBSCAN here, because this takes requires a long time and the trend is already visible</span>
            <span class="k">continue</span>
        <span class="n">start_time</span> <span class="o">=</span> <span class="n">timeit</span><span class="o">.</span><span class="n">default_timer</span><span class="p">()</span>
        <span class="n">cl</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">elapsed</span> <span class="o">=</span> <span class="n">timeit</span><span class="o">.</span><span class="n">default_timer</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>
        <span class="n">runtime</span><span class="p">[</span><span class="n">cl_cnt</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">elapsed</span><span class="p">)</span>
        
<span class="n">fix</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">instances</span><span class="p">,</span> <span class="n">runtime</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="n">names</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">instances</span><span class="p">,</span> <span class="n">runtime</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="n">names</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">instances</span><span class="p">[:</span><span class="nb">len</span><span class="p">(</span><span class="n">runtime</span><span class="p">[</span><span class="mi">2</span><span class="p">])],</span> <span class="n">runtime</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="n">names</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">instances</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Laufzeit in Sekunden&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Anzahl der Instanzen&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/kapitel_06_37_0.png" src="../_images/kapitel_06_37_0.png" />
</div>
</div>
<p>Wie man sieht, ist die Laufzeit des <span class="math notranslate nohighlight">\(k\)</span>-Mean und des EM-Algorithmus ähnlich, DBSCAN ist langsamer. Dies liegt daran, dass es bei vielen Instanzen möglicherweise sehr große Nachbarschaften gibt, was sich negativ auf die benötigte Rechenzeit von DBSCAN auswirkt. Die Laufzeitmessungen hängen aber auch von den Daten ab. Wenn die Daten besonders dünn (engl. <em>sparse</em>) oder dicht sind oder es eine seltsame Verteilung gibt, kann sich dies auf die Laufzeiten auswirken. Im Allgemeinen ist die Laufzeit aber ein sekundärer Aspekt bei der Auswahl eines geeigneten Clusteralgorithmus, es sei denn, man hat extrem viele Instanzen.</p>
</div>
<div class="section" id="interpretierbarkeit-und-darstellung">
<h3><span class="section-number">6.7.4. </span>Interpretierbarkeit und Darstellung<a class="headerlink" href="#interpretierbarkeit-und-darstellung" title="Permalink to this headline">¶</a></h3>
<p>Ein großer Vorteil des <span class="math notranslate nohighlight">\(k\)</span>-Means und des EM-Clustering ist, dass diese Verfahren eine gut interpretierbare und kompakte mathematische Beschreibung der Cluster liefern, indem die Cluster durch die Centroids bzw. Normalverteilungen beschrieben werden. Dies liefert uns ein mächtiges Werkzeug, um die Bedeutung der Cluster zu verstehen. Außerdem ist es hierdurch möglich, unabhängig von den anderen Daten zu bestimmen, zu welchem Cluster eine Instanz gehört. Hierdurch lassen sich die Ergebnisse problemlos auf anderen Systemen einsetzen, wo die Daten selbst nicht zur Verfügung stehen.</p>
<p>DBSCAN und SLINK habe keine vergleichbare Darstellung. Stattdessen sind hier die Instanzen selbst die Beschreibung der Cluster. Hierdurch kann man die Ergebnisse nur visuell interpretieren, was gerade bei hochdimensionalen Daten äußert schwierig ist. SLINK ist für die Interpretation durch die Dendrogramme etwas besser, da man zumindest relativ einfach beurteilen kann, wie groß die Abstände zwischen den Instanzen bzw. den Clustern sind.</p>
</div>
<div class="section" id="kategorische-merkmale">
<h3><span class="section-number">6.7.5. </span>Kategorische Merkmale<a class="headerlink" href="#kategorische-merkmale" title="Permalink to this headline">¶</a></h3>
<p>Kategorische Merkmale sind ein Problem für alle Algorithmen, die wir betrachtet haben. Sowohl <span class="math notranslate nohighlight">\(k\)</span>-Means als auch DBSCAN und SLINK berechnen Distanzen. Da man per Definition keine sinnvollen Distanzen zwischen zwei Kategorien ermitteln kann, kann man auch keine sinnvollen Distanzen zwischen Instanzen mit kategorischen Daten berechnen. Im Fall von <span class="math notranslate nohighlight">\(k\)</span>-Means gibt es eine Variante namens <span class="math notranslate nohighlight">\(k\)</span>-Modes, bei der die Centroids durch den Modus der Daten im Cluster berechnet werden. Beim EM-Clustering geht die Normalverteilung von einem kontinuierlichen Wertebereich aus, was bei kategorischen Daten nicht der Fall ist. Die Lösung hier besteht darin, keine Normalverteilung für die Beschreibung der Cluster zu verwenden, sondern stattdessen den EM-Algorithmus mit einer besser passenden Verteilung zu benutzen, zum Beispiel einer Multinomialverteilung.</p>
</div>
<div class="section" id="fehlende-merkmale">
<h3><span class="section-number">6.7.6. </span>Fehlende Merkmale<a class="headerlink" href="#fehlende-merkmale" title="Permalink to this headline">¶</a></h3>
<p>In der Praxis kommt es häufig vor, dass es Instanzen gibt, bei denen nicht die Werte für alle Merkmale bekannt sind. Man spricht hier von fehlenden Werten bzw. fehlenden Merkmalen. Es gibt drei Möglichkeiten, mit fehlenden Merkmalen umzugehen: Entweder kann der Algorithmus von sich aus mit unvollständigen Daten arbeiten. Dies ist, bis auf wenige Ausnahmen, nicht der Fall. Die zweite Möglichkeit besteht darin, alle unvollständigen Instanzen zu entfernen. Hier hat man jedoch möglicherweise in der Operationalisierung ein Problem. Als dritte Möglichkeit kann man durch <em>Imputation</em> eine Schätzung für die fehlenden Werte berechnen, zum Beispiel durch eine Regression basierend auf den vorhandenen Daten. Details zur Imputation finden Sie in der Literatur, zum Beispiel bei Barnard und Meng <a class="footnote-reference brackets" href="#barnard" id="id6">3</a>.</p>
</div>
<div class="section" id="korrelierte-merkmale">
<h3><span class="section-number">6.7.7. </span>Korrelierte Merkmale<a class="headerlink" href="#korrelierte-merkmale" title="Permalink to this headline">¶</a></h3>
<p>Korrelierte Merkmale sind für alle betrachteten Algorithmen problematisch, insbesondere die Algorithmen <span class="math notranslate nohighlight">\(k\)</span>-Means, DBSCAN und SLINK, die Distanzen berechnen, haben hierdurch ein Problem mit Korrelationen. Um dies zu verdeutlichen, betrachten wir einen extremen Fall: Nehmen wir an, es gebe zwei Merkmale, das normalisierte Alter in Jahren und das normalisierte Alter in Tagen. Man spricht von einem Merkmal als <em>normalisiert</em> (bzw. <em>normiert</em>), wenn die Werte auf das Intervall <span class="math notranslate nohighlight">\([0,1]\)</span>  reskaliert wurden. Die Werte dieser beiden Features wären nicht identisch, da das Alter in Tagen feingranularer ist als das Alter in Jahren. Die Bedeutung und der Informationsgehalt wären jedoch nahezu identisch, sodass es faktisch das Merkmal “normalisiertes Alter” zwei Mal geben würde. Das heißt auch, dass das Alter doppelt in die Distanzberechnung einfließt und dementsprechend einen überproportionalen Einfluss auf die Distanz hätte. Je mehr und je stärker die Korrelationen in den Daten sind, desto ausgeprägter ist der negative Effekt auf die Distanzberechnungen.</p>
<p>Das EM-Clustering ist robuster gegenüber Korrelationen, da diese durch die Kovarianzen berücksichtigt werden können. Da man jedoch möglicherweise mehr Merkmale als benötigt hat, wirken sich Korrelationen auf die Berechnung des BIC aus, weshalb man eventuell eine zu kleine Anzahl an Clustern auswählt, da die Komplexität des Ergebnisses überschätzt wird.</p>
</div>
<div class="section" id="zusammenfassung-des-vergleichs">
<h3><span class="section-number">6.7.8. </span>Zusammenfassung des Vergleichs<a class="headerlink" href="#zusammenfassung-des-vergleichs" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="#tbl-cluster1"><span class="std std-numref">Table 6.1</span></a> und <a class="reference internal" href="#tbl-cluster2"><span class="std std-numref">Table 6.2</span></a> fassen die Stärken und Schwächen der Clusteralgorithmen noch einmal zusammen.</p>
<table class="colwidths-auto table" id="tbl-cluster1">
<caption><span class="caption-number">Table 6.1 </span><span class="caption-text">Zusammenfassung der Vor- und Nachteile der Clusteralgorithmen (1/2)</span><a class="headerlink" href="#tbl-cluster1" title="Permalink to this table">¶</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p><span class="xref myst"></span></p></th>
<th class="head"><p>Clusterform</p></th>
<th class="head"><p>Clusteranzahl</p></th>
<th class="head"><p>Laufzeit</p></th>
<th class="head"><p>Interpretierbarkeit</p></th>
<th class="head"><p>Darstellung</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(k\)</span>-Means</p></td>
<td><p><span class="math notranslate nohighlight">\(-\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-\)</span></p></td>
<td><p>o</p></td>
<td><p><span class="math notranslate nohighlight">\(+\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(+\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>EM-Clustering</p></td>
<td><p>o</p></td>
<td><p>o</p></td>
<td><p>o</p></td>
<td><p><span class="math notranslate nohighlight">\(+\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(+\)</span></p></td>
</tr>
<tr class="row-even"><td><p>DBSCAN</p></td>
<td><p><span class="math notranslate nohighlight">\(+\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(+\)</span></p></td>
<td><p>o</p></td>
<td><p><span class="math notranslate nohighlight">\(-\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>SLINK</p></td>
<td><p><span class="math notranslate nohighlight">\(+\)</span></p></td>
<td><p>o</p></td>
<td><p>o (*)</p></td>
<td><p><span class="math notranslate nohighlight">\(+\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-\)</span></p></td>
</tr>
</tbody>
</table>
<table class="colwidths-auto table" id="tbl-cluster2">
<caption><span class="caption-number">Table 6.2 </span><span class="caption-text">Zusammenfassung der Vor- und Nachteile der Clusteralgorithmen (2/2)</span><a class="headerlink" href="#tbl-cluster2" title="Permalink to this table">¶</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p><span class="xref myst"></span></p></th>
<th class="head"><p>Kategorische Merkmale</p></th>
<th class="head"><p>Fehlende Merkmale</p></th>
<th class="head"><p>Korrelierte Merkmale</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(k\)</span>-Means</p></td>
<td><p><span class="math notranslate nohighlight">\(-\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>EM-Clustering</p></td>
<td><p><span class="math notranslate nohighlight">\(-\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-\)</span></p></td>
<td><p>o</p></td>
</tr>
<tr class="row-even"><td><p>DBSCAN</p></td>
<td><p><span class="math notranslate nohighlight">\(-\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>SLINK</p></td>
<td><p><span class="math notranslate nohighlight">\(-\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-\)</span></p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="ubung">
<h2><span class="section-number">6.8. </span>Übung<a class="headerlink" href="#ubung" title="Permalink to this headline">¶</a></h2>
<p>In dieser Übung wollen wir das Clustering vertiefen. Das Ziel ist es, die Parameterauswahl und die Interpretation der Cluster bei hochdimensionalen Daten zu üben. Die Interpretation ist auch die größte Schwierigkeit dieser Übung.
Als Daten verwenden wir die Bostondaten. Hier noch ein paar Tipps vorweg:</p>
<ul class="simple">
<li><p>Denken Sie an die Wertebereiche der Merkmale. Diese könnten einen Einfluss auf die Ergebnisse haben.</p></li>
<li><p>Durch die Dimensionalität ist es schwierig, die Cluster zu interpretieren. Setzen Sie die PCA und paarweise Scatterplots ein (siehe <a class="reference internal" href="kapitel_04.html"><span class="doc std std-doc">Kapitel 4</span></a>). Beide haben gewisse Vor- und Nachteile, auf die Sie hier auch achten können.</p></li>
</ul>
<p>In jedem der folgenden Aufgabenteile sollten Sie probieren, die Bedeutung jedes Clusters zu bestimmen, also eine Beschreibung der Instanzen, die gemeinsam gruppiert werden, als eine Art Gruppenname.</p>
<div class="section" id="id7">
<h3><span class="section-number">6.8.1. </span><span class="math notranslate nohighlight">\(k\)</span>-Means-Algorithmus<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h3>
<p>Benutzen Sie den <span class="math notranslate nohighlight">\(k\)</span>-Means-Algorithmus, um die Daten zu clustern. Bestimmen Sie eine geeignete Clusteranzahl <span class="math notranslate nohighlight">\(k\)</span>. Benutzen Sie Ihr Wissen über die Daten, Visualisierungen und WSS, um eine geeignete Anzahl Cluster zu bestimmen.</p>
</div>
<div class="section" id="id8">
<h3><span class="section-number">6.8.2. </span>EM Clustering<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h3>
<p>Benutzen Sie das EM-Clustering, um die Daten zu clustern. Bestimmen Sie eine geeignete Clusteranzahl <span class="math notranslate nohighlight">\(k\)</span>. Benutzen Sie Ihr Wissen über die Daten, Visualisierungen und BIC, um eine geeignete Anzahl Cluster zu bestimmen.</p>
</div>
<div class="section" id="id9">
<h3><span class="section-number">6.8.3. </span>DBSCAN<a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h3>
<p>Benutzen Sie DBSCAN, um die Daten zu clustern. Bestimmen Sie geeignete Werte für <span class="math notranslate nohighlight">\(\epsilon\)</span> und <span class="math notranslate nohighlight">\(minPts\)</span>. Benutzen Sie Ihr Wissen über die Daten und Visualisierungen, um geeignete Werte zu finden.</p>
</div>
<div class="section" id="slink">
<h3><span class="section-number">6.8.4. </span>SLINK<a class="headerlink" href="#slink" title="Permalink to this headline">¶</a></h3>
<p>Benutzen Sie den SLINK-Algorithmus, um die Daten zu clustern. Bestimmen Sie mithilfe eines Dendrogramms einen geeigneten Grenzwert für die Level.</p>
</div>
<div class="section" id="vergleichen-sie-die-ergebnisse">
<h3><span class="section-number">6.8.5. </span>Vergleichen Sie die Ergebnisse<a class="headerlink" href="#vergleichen-sie-die-ergebnisse" title="Permalink to this headline">¶</a></h3>
<p>Vergleichen Sie die Ergebnisse der Algorithmen. Welche Unterschiede gibt es zum Beispiel in der Clusteranzahl und der Form der Cluster? Identifizieren Sie eventuelle Probleme der Algorithmen.</p>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="voronoi"><span class="brackets"><a class="fn-backref" href="#id2">1</a></span></dt>
<dd><p><a class="reference external" href="https://mathworld.wolfram.com/VoronoiDiagram.html">https://mathworld.wolfram.com/VoronoiDiagram.html</a></p>
</dd>
<dt class="label" id="occam"><span class="brackets"><a class="fn-backref" href="#id4">2</a></span></dt>
<dd><p><a class="reference external" href="https://doi.org/10.1007/978-1-4899-7687-1_614">https://doi.org/10.1007/978-1-4899-7687-1_614</a></p>
</dd>
<dt class="label" id="barnard"><span class="brackets"><a class="fn-backref" href="#id6">3</a></span></dt>
<dd><p><a class="reference external" href="https://doi.org/10.1177%2F096228029900800103">https://doi.org/10.1177/096228029900800103</a></p>
</dd>
</dl>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./chapters"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="kapitel_05.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title"><span class="section-number">5. </span>Assoziationsregeln</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="kapitel_07.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title"><span class="section-number">7. </span>Klassifikation</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Steffen Herbold<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>